# Matrix Calculus & Gradients Exercises - Module 2

**Time:** 3-4 hours
**Difficulty:** Intermediate
**Materials needed:** Paper, pencil, calculator

Complete these by hand to build intuition for backpropagation. Solutions in `guides/solutions/calculus_gradients_solutions.md`

---

## Part 1: Scalar Derivatives Review (30 min)

### Exercise 1.1: Basic Derivatives
Calculate derivatives by hand (show your work):

1. f(x) = 3xÂ² + 2x - 5,  find f'(x)
2. f(x) = xÂ³ - 4xÂ² + x,  find f'(x)
3. f(x) = 1/xÂ²,  find f'(x)
4. f(x) = e^(2x),  find f'(x)
5. f(x) = ln(xÂ²),  find f'(x)

### Exercise 1.2: Chain Rule
Calculate using chain rule:

1. f(x) = (3x + 2)â´,  find f'(x)
2. f(x) = e^(xÂ²),  find f'(x)
3. f(x) = ln(2x + 1),  find f'(x)
4. f(x) = sin(3xÂ²),  find f'(x)

### Exercise 1.3: Product and Quotient Rules
1. f(x) = xÂ² Â· e^x,  find f'(x) using product rule
2. f(x) = xÂ³ Â· ln(x),  find f'(x)
3. f(x) = (xÂ² + 1)/(x - 1),  find f'(x) using quotient rule

---

## Part 2: Partial Derivatives (45 min)

### Exercise 2.1: Basic Partial Derivatives
For f(x, y) = xÂ²y + 3xyÂ² - 2x + y

Calculate:
1. âˆ‚f/âˆ‚x (treat y as constant)
2. âˆ‚f/âˆ‚y (treat x as constant)
3. Evaluate both at point (x, y) = (1, 2)

### Exercise 2.2: More Partial Derivatives
For f(x, y) = e^(xy) + xÂ²yÂ³

Calculate:
1. âˆ‚f/âˆ‚x
2. âˆ‚f/âˆ‚y
3. Evaluate at (x, y) = (0, 1)

### Exercise 2.3: Second-Order Partial Derivatives
For f(x, y) = xÂ³yÂ² - 2xy + 5

Calculate all second-order partials:
1. âˆ‚Â²f/âˆ‚xÂ²
2. âˆ‚Â²f/âˆ‚yÂ²
3. âˆ‚Â²f/âˆ‚xâˆ‚y
4. âˆ‚Â²f/âˆ‚yâˆ‚x
5. Verify that âˆ‚Â²f/âˆ‚xâˆ‚y = âˆ‚Â²f/âˆ‚yâˆ‚x (Clairaut's theorem)

---

## Part 3: Gradients (60 min)

### Exercise 3.1: Computing Gradients
For f(x, y) = xÂ² + yÂ² - 2x - 4y + 5

1. Calculate gradient: âˆ‡f = [âˆ‚f/âˆ‚x, âˆ‚f/âˆ‚y]áµ€
2. Find the gradient at point (1, 2)
3. Find critical points (where âˆ‡f = 0)
4. Is the critical point a minimum, maximum, or saddle point?

### Exercise 3.2: Gradient of Quadratic Form
For f(x) = xáµ€Ax where x = [xâ‚, xâ‚‚]áµ€ and A = [[2, 1], [1, 3]]

1. Expand f(x) in terms of xâ‚, xâ‚‚
2. Calculate âˆ‚f/âˆ‚xâ‚
3. Calculate âˆ‚f/âˆ‚xâ‚‚
4. Write the gradient âˆ‡f(x)
5. Verify the formula: âˆ‡f(x) = (A + Aáµ€)x

### Exercise 3.3: Gradient Descent Step
Given f(x, y) = xÂ² + 4yÂ² and starting point (xâ‚€, yâ‚€) = (4, 2):

1. Calculate gradient at (4, 2)
2. Using learning rate Î± = 0.1, calculate one gradient descent step:
   (xâ‚, yâ‚) = (xâ‚€, yâ‚€) - Î±âˆ‡f(xâ‚€, yâ‚€)
3. Calculate f(xâ‚€, yâ‚€) and f(xâ‚, yâ‚) - did we reduce the function?
4. Calculate the gradient at the new point (xâ‚, yâ‚)

---

## Part 4: Chain Rule for Multivariable Functions (60 min)

### Exercise 4.1: Simple Chain Rule
Let z = f(x, y) = xÂ² + yÂ² where x = 2t and y = 3t

Find dz/dt using chain rule:
dz/dt = (âˆ‚f/âˆ‚x)(dx/dt) + (âˆ‚f/âˆ‚y)(dy/dt)

### Exercise 4.2: Backpropagation Example
Consider a simple neural network computation:

```
Input: x
Hidden: h = Ïƒ(wx + b)  where Ïƒ(z) = 1/(1 + e^(-z))
Output: y = hÂ²
Loss: L = (y - t)Â²  where t is target
```

Given x = 2, w = 0.5, b = 1, t = 0.8:

1. **Forward pass:** Calculate h, y, L (show all steps)

2. **Backward pass:** Calculate gradients using chain rule:
   - âˆ‚L/âˆ‚y
   - âˆ‚y/âˆ‚h
   - âˆ‚h/âˆ‚w
   - âˆ‚h/âˆ‚b
   - âˆ‚L/âˆ‚w = (âˆ‚L/âˆ‚y)(âˆ‚y/âˆ‚h)(âˆ‚h/âˆ‚w)
   - âˆ‚L/âˆ‚b = (âˆ‚L/âˆ‚y)(âˆ‚y/âˆ‚h)(âˆ‚h/âˆ‚b)

Note: Ïƒ'(z) = Ïƒ(z)(1 - Ïƒ(z))

### Exercise 4.3: Vector Chain Rule
Given:
- z = f(y) = yâ‚Â² + yâ‚‚Â²
- y = g(x) = [2xâ‚ + xâ‚‚, xâ‚ - xâ‚‚]áµ€

Find âˆ‚z/âˆ‚xâ‚ and âˆ‚z/âˆ‚xâ‚‚ using chain rule:
âˆ‚z/âˆ‚xáµ¢ = Î£â±¼ (âˆ‚z/âˆ‚yâ±¼)(âˆ‚yâ±¼/âˆ‚xáµ¢)

---

## Part 5: Jacobian Matrices (45 min)

### Exercise 5.1: Computing Jacobian
For function f: â„Â² â†’ â„Â³ defined by:
```
fâ‚(xâ‚, xâ‚‚) = xâ‚Â² + xâ‚‚
fâ‚‚(xâ‚, xâ‚‚) = xâ‚xâ‚‚
fâ‚ƒ(xâ‚, xâ‚‚) = xâ‚ + 2xâ‚‚Â²
```

Calculate the Jacobian matrix:
```
J = [[âˆ‚fâ‚/âˆ‚xâ‚, âˆ‚fâ‚/âˆ‚xâ‚‚],
     [âˆ‚fâ‚‚/âˆ‚xâ‚, âˆ‚fâ‚‚/âˆ‚xâ‚‚],
     [âˆ‚fâ‚ƒ/âˆ‚xâ‚, âˆ‚fâ‚ƒ/âˆ‚xâ‚‚]]
```

Evaluate at point (1, 2).

### Exercise 5.2: Chain Rule with Jacobians
Given:
- z = f(y): â„Â² â†’ â„ where f(yâ‚, yâ‚‚) = yâ‚Â² + 2yâ‚‚Â²
- y = g(x): â„Â³ â†’ â„Â² where g(xâ‚, xâ‚‚, xâ‚ƒ) = [xâ‚ + xâ‚‚, xâ‚‚xâ‚ƒ]áµ€

1. Calculate âˆ‡f (gradient of f)
2. Calculate Jacobian of g: Jg (2Ã—3 matrix)
3. Calculate gradient of z with respect to x using: âˆ‡â‚“z = Jgáµ€âˆ‡f

---

## Part 6: Hessian Matrices (30 min)

### Exercise 6.1: Computing Hessian
For f(x, y) = xÂ³ + yÂ³ - 3xy

Calculate the Hessian matrix:
```
H = [[âˆ‚Â²f/âˆ‚xÂ², âˆ‚Â²f/âˆ‚xâˆ‚y],
     [âˆ‚Â²f/âˆ‚yâˆ‚x, âˆ‚Â²f/âˆ‚yÂ²]]
```

Evaluate at point (1, 1).

### Exercise 6.2: Analyzing Critical Points
For f(x, y) = xÂ² - xy + yÂ² + 2x - y

1. Find critical points (solve âˆ‡f = 0)
2. Calculate Hessian at each critical point
3. Determine nature of each critical point using second derivative test:
   - If det(H) > 0 and âˆ‚Â²f/âˆ‚xÂ² > 0: local minimum
   - If det(H) > 0 and âˆ‚Â²f/âˆ‚xÂ² < 0: local maximum
   - If det(H) < 0: saddle point

---

## Part 7: ML-Specific Gradients (60 min)

### Exercise 7.1: Linear Regression Gradient
For linear regression: Å· = wx + b
Loss: L = (y - Å·)Â² = (y - wx - b)Â²

Given data point: x = 3, y = 7
Current parameters: w = 1.5, b = 2

1. Calculate predicted value Å·
2. Calculate loss L
3. Calculate âˆ‚L/âˆ‚w (show chain rule steps)
4. Calculate âˆ‚L/âˆ‚b
5. Update parameters using Î± = 0.1:
   - w_new = w - Î±(âˆ‚L/âˆ‚w)
   - b_new = b - Î±(âˆ‚L/âˆ‚b)

### Exercise 7.2: Logistic Regression Gradient
For binary classification:
- Prediction: Å· = Ïƒ(wx + b) where Ïƒ(z) = 1/(1 + e^(-z))
- Binary cross-entropy loss: L = -[y log(Å·) + (1-y) log(1-Å·)]

Given: x = 2, y = 1 (true class), w = 0.5, b = 0.5

1. Calculate z = wx + b
2. Calculate Å· = Ïƒ(z)
3. Calculate loss L
4. Calculate âˆ‚L/âˆ‚Å·
5. Calculate âˆ‚Å·/âˆ‚z (remember: Ïƒ'(z) = Ïƒ(z)(1-Ïƒ(z)))
6. Calculate âˆ‚z/âˆ‚w and âˆ‚z/âˆ‚b
7. Use chain rule: âˆ‚L/âˆ‚w = (âˆ‚L/âˆ‚Å·)(âˆ‚Å·/âˆ‚z)(âˆ‚z/âˆ‚w)
8. Calculate âˆ‚L/âˆ‚b similarly

### Exercise 7.3: Softmax Gradient (Challenging)
For multi-class classification with 3 classes:
```
Logits: z = [zâ‚, zâ‚‚, zâ‚ƒ]áµ€
Softmax: Å·áµ¢ = e^(záµ¢) / Î£â±¼e^(zâ±¼)
Cross-entropy: L = -Î£áµ¢ yáµ¢ log(Å·áµ¢)
```

Given: z = [2, 1, 0.5], y = [1, 0, 0] (one-hot encoded, class 0 is correct)

1. Calculate softmax outputs Å·â‚, Å·â‚‚, Å·â‚ƒ
2. Calculate loss L
3. Show that âˆ‚L/âˆ‚záµ¢ = Å·áµ¢ - yáµ¢ (this is a remarkable simplification!)
4. Calculate gradients âˆ‚L/âˆ‚zâ‚, âˆ‚L/âˆ‚zâ‚‚, âˆ‚L/âˆ‚zâ‚ƒ

---

## NumPy Verification

After solving by hand, verify with numerical gradients:

```python
import numpy as np

def numerical_gradient(f, x, h=1e-5):
    """Compute gradient numerically using finite differences"""
    grad = np.zeros_like(x)
    for i in range(len(x)):
        x_plus_h = x.copy()
        x_plus_h[i] += h
        x_minus_h = x.copy()
        x_minus_h[i] -= h
        grad[i] = (f(x_plus_h) - f(x_minus_h)) / (2*h)
    return grad

# Example: verify gradient of f(x,y) = xÂ² + yÂ²
def f(x):
    return x[0]**2 + x[1]**2

x = np.array([1.0, 2.0])
numerical_grad = numerical_gradient(f, x)
analytical_grad = 2*x  # True gradient

print("Numerical:", numerical_grad)
print("Analytical:", analytical_grad)
print("Difference:", np.linalg.norm(numerical_grad - analytical_grad))
```

---

## Challenge Problems

### Challenge 1: Newton's Method
Implement one step of Newton's method for minimizing f(x, y) = xÂ² + 4yÂ²

Formula: x_new = x - Hâ»Â¹âˆ‡f

Starting from (4, 2), calculate the next point.

### Challenge 2: Batch Gradient
Extend Exercise 7.1 to mini-batch of 3 points:
- (xâ‚, yâ‚) = (1, 3)
- (xâ‚‚, yâ‚‚) = (2, 5)
- (xâ‚ƒ, yâ‚ƒ) = (3, 7)

Calculate average gradient over the batch.

### Challenge 3: Derive Backprop for 2-Layer Network
```
x â†’ hâ‚ = Ïƒ(Wâ‚x + bâ‚) â†’ hâ‚‚ = Ïƒ(Wâ‚‚hâ‚ + bâ‚‚) â†’ L = (hâ‚‚ - y)Â²
```

Derive âˆ‚L/âˆ‚Wâ‚, âˆ‚L/âˆ‚bâ‚, âˆ‚L/âˆ‚Wâ‚‚, âˆ‚L/âˆ‚bâ‚‚ using chain rule.

---

## Submission Checklist

- [ ] All partial derivatives calculated correctly
- [ ] Chain rule applied properly for backprop
- [ ] Gradients verified numerically in NumPy
- [ ] Understanding of gradient descent update rule
- [ ] Can explain backpropagation intuitively

---

## Key Concepts

1. **Gradient** points in direction of steepest ascent
2. **Negative gradient** points toward local minimum
3. **Chain rule** is the foundation of backpropagation
4. **Jacobian** generalizes derivative to vector functions
5. **Hessian** describes local curvature (second derivatives)
6. **Numerical gradients** are slower but useful for verification

Mastering these concepts is essential for understanding deep learning! ğŸ§ 
