<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Matrix Calculus & Gradients Exercises - ML Roadmap</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        .exercise-item {
            border-left: 4px solid #e5e7eb;
            padding-left: 1rem;
            margin-bottom: 1.5rem;
            transition: all 0.2s;
        }
        .exercise-item.completed {
            border-left-color: #10b981;
            opacity: 0.7;
        }
        .exercise-item h3 {
            font-size: 1.25rem;
            font-weight: 600;
            color: #374151;
            margin-bottom: 0.5rem;
        }
        .exercise-item.completed h3 {
            text-decoration: line-through;
            color: #9ca3af;
        }
        .section-header {
            font-size: 1.5rem;
            font-weight: 700;
            color: #1f2937;
            margin-top: 2rem;
            margin-bottom: 1rem;
            border-bottom: 2px solid #10b981;
            padding-bottom: 0.5rem;
        }
        .content-text {
            color: #4b5563;
            line-height: 1.6;
            margin-bottom: 0.75rem;
        }
        .content-text.completed {
            color: #9ca3af;
        }
        pre {
            background-color: #1f2937;
            color: #f3f4f6;
            padding: 1rem;
            border-radius: 0.5rem;
            overflow-x: auto;
            margin: 1rem 0;
        }
        code {
            font-family: 'Courier New', monospace;
        }
        .checkbox-custom {
            width: 1.25rem;
            height: 1.25rem;
            cursor: pointer;
            accent-color: #10b981;
        }
    </style>
</head>
<body class="bg-gradient-to-br from-emerald-50 via-teal-50 to-green-50 min-h-screen">
    <div class="max-w-5xl mx-auto p-4 md:p-8">
        <!-- Header -->
        <div class="bg-white rounded-lg shadow-lg p-6 mb-6">
            <div class="flex items-center justify-between mb-4">
                <h1 class="text-3xl font-bold text-gray-800">Matrix Calculus & Gradients Exercises</h1>
                <div class="flex space-x-2">
                    <a href="../solutions/calculus_gradients_solutions.html" class="px-4 py-2 bg-emerald-600 hover:bg-emerald-700 text-white rounded-lg text-sm font-semibold">
                        View Solutions
                    </a>
                    <a href="../../index.html" class="px-4 py-2 bg-indigo-600 hover:bg-indigo-700 text-white rounded-lg text-sm font-semibold">
                        ‚Üê Back to Roadmap
                    </a>
                </div>
            </div>
            <div class="flex items-center justify-between">
                <div class="flex items-center space-x-4 text-sm text-gray-600">
                    <span>‚è±Ô∏è Time: 3-4 hours</span>
                    <span>üìä Difficulty: Intermediate</span>
                </div>
                <div class="text-right">
                    <div class="text-sm text-gray-600 mb-1">Progress: <span id="progress-text" class="font-bold text-emerald-600">0%</span></div>
                    <div class="w-48 bg-gray-200 rounded-full h-2">
                        <div id="progress-bar" class="bg-emerald-600 h-2 rounded-full transition-all" style="width: 0%"></div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Content -->
        <div class="bg-white rounded-lg shadow-lg p-8">
            <div class="mb-6 p-4 bg-blue-50 border-l-4 border-blue-500 rounded">
                <p class="text-sm text-gray-700">
                    <strong>Instructions:</strong> Complete these exercises by hand to build intuition for backpropagation.
                    Check off each exercise as you complete it to track your progress.
                </p>
            </div>

            <h2 class="section-header">Part 1: Scalar Derivatives Review (30 min)</h2>

            <div class="exercise-item" data-exercise-id="calculus-gradients-1.1">
                <label class="flex items-start space-x-3 cursor-pointer">
                    <input type="checkbox" class="checkbox-custom mt-1" onchange="toggleExercise('calculus-gradients-1.1')">
                    <div class="flex-1">
                        <h3>Exercise 1.1: Basic Derivatives</h3>
                        <div class="content-text">
                            Calculate derivatives by hand (show your work):
                            <ol class="list-decimal ml-6 my-2">
                                <li>f(x) = 3x¬≤ + 2x - 5, find f'(x)</li>
                                <li>f(x) = x¬≥ - 4x¬≤ + x, find f'(x)</li>
                                <li>f(x) = 1/x¬≤, find f'(x)</li>
                                <li>f(x) = e^(2x), find f'(x)</li>
                                <li>f(x) = ln(x¬≤), find f'(x)</li>
                            </ol>
                        </div>
                    </div>
                </label>
            </div>

            <div class="exercise-item" data-exercise-id="calculus-gradients-1.2">
                <label class="flex items-start space-x-3 cursor-pointer">
                    <input type="checkbox" class="checkbox-custom mt-1" onchange="toggleExercise('calculus-gradients-1.2')">
                    <div class="flex-1">
                        <h3>Exercise 1.2: Chain Rule</h3>
                        <div class="content-text">
                            Calculate using chain rule:
                            <ol class="list-decimal ml-6 my-2">
                                <li>f(x) = (3x + 2)‚Å¥, find f'(x)</li>
                                <li>f(x) = e^(x¬≤), find f'(x)</li>
                                <li>f(x) = ln(2x + 1), find f'(x)</li>
                                <li>f(x) = sin(3x¬≤), find f'(x)</li>
                            </ol>
                        </div>
                    </div>
                </label>
            </div>

            <div class="exercise-item" data-exercise-id="calculus-gradients-1.3">
                <label class="flex items-start space-x-3 cursor-pointer">
                    <input type="checkbox" class="checkbox-custom mt-1" onchange="toggleExercise('calculus-gradients-1.3')">
                    <div class="flex-1">
                        <h3>Exercise 1.3: Product and Quotient Rules</h3>
                        <div class="content-text">
                            <ol class="list-decimal ml-6">
                                <li>f(x) = x¬≤ ¬∑ e^x, find f'(x) using product rule</li>
                                <li>f(x) = x¬≥ ¬∑ ln(x), find f'(x)</li>
                                <li>f(x) = (x¬≤ + 1)/(x - 1), find f'(x) using quotient rule</li>
                            </ol>
                        </div>
                    </div>
                </label>
            </div>

            <h2 class="section-header">Part 2: Partial Derivatives (45 min)</h2>

            <div class="exercise-item" data-exercise-id="calculus-gradients-2.1">
                <label class="flex items-start space-x-3 cursor-pointer">
                    <input type="checkbox" class="checkbox-custom mt-1" onchange="toggleExercise('calculus-gradients-2.1')">
                    <div class="flex-1">
                        <h3>Exercise 2.1: Basic Partial Derivatives</h3>
                        <div class="content-text">
                            For f(x, y) = x¬≤y + 3xy¬≤ - 2x + y<br><br>
                            Calculate:
                            <ol class="list-decimal ml-6">
                                <li>‚àÇf/‚àÇx (treat y as constant)</li>
                                <li>‚àÇf/‚àÇy (treat x as constant)</li>
                                <li>Evaluate both at point (x, y) = (1, 2)</li>
                            </ol>
                        </div>
                    </div>
                </label>
            </div>

            <div class="exercise-item" data-exercise-id="calculus-gradients-2.2">
                <label class="flex items-start space-x-3 cursor-pointer">
                    <input type="checkbox" class="checkbox-custom mt-1" onchange="toggleExercise('calculus-gradients-2.2')">
                    <div class="flex-1">
                        <h3>Exercise 2.2: More Partial Derivatives</h3>
                        <div class="content-text">
                            For f(x, y) = e^(xy) + x¬≤y¬≥<br><br>
                            Calculate:
                            <ol class="list-decimal ml-6">
                                <li>‚àÇf/‚àÇx</li>
                                <li>‚àÇf/‚àÇy</li>
                                <li>Evaluate at (x, y) = (0, 1)</li>
                            </ol>
                        </div>
                    </div>
                </label>
            </div>

            <div class="exercise-item" data-exercise-id="calculus-gradients-2.3">
                <label class="flex items-start space-x-3 cursor-pointer">
                    <input type="checkbox" class="checkbox-custom mt-1" onchange="toggleExercise('calculus-gradients-2.3')">
                    <div class="flex-1">
                        <h3>Exercise 2.3: Second-Order Partial Derivatives</h3>
                        <div class="content-text">
                            For f(x, y) = x¬≥y¬≤ - 2xy + 5<br><br>
                            Calculate all second-order partials:
                            <ol class="list-decimal ml-6">
                                <li>‚àÇ¬≤f/‚àÇx¬≤</li>
                                <li>‚àÇ¬≤f/‚àÇy¬≤</li>
                                <li>‚àÇ¬≤f/‚àÇx‚àÇy</li>
                                <li>‚àÇ¬≤f/‚àÇy‚àÇx</li>
                                <li>Verify that ‚àÇ¬≤f/‚àÇx‚àÇy = ‚àÇ¬≤f/‚àÇy‚àÇx (Clairaut's theorem)</li>
                            </ol>
                        </div>
                    </div>
                </label>
            </div>

            <h2 class="section-header">Part 3: Gradients (60 min)</h2>

            <div class="exercise-item" data-exercise-id="calculus-gradients-3.1">
                <label class="flex items-start space-x-3 cursor-pointer">
                    <input type="checkbox" class="checkbox-custom mt-1" onchange="toggleExercise('calculus-gradients-3.1')">
                    <div class="flex-1">
                        <h3>Exercise 3.1: Computing Gradients</h3>
                        <div class="content-text">
                            For f(x, y) = x¬≤ + y¬≤ - 2x - 4y + 5
                            <ol class="list-decimal ml-6 my-2">
                                <li>Calculate gradient: ‚àáf = [‚àÇf/‚àÇx, ‚àÇf/‚àÇy]·µÄ</li>
                                <li>Find the gradient at point (1, 2)</li>
                                <li>Find critical points (where ‚àáf = 0)</li>
                                <li>Is the critical point a minimum, maximum, or saddle point?</li>
                            </ol>
                        </div>
                    </div>
                </label>
            </div>

            <div class="exercise-item" data-exercise-id="calculus-gradients-3.2">
                <label class="flex items-start space-x-3 cursor-pointer">
                    <input type="checkbox" class="checkbox-custom mt-1" onchange="toggleExercise('calculus-gradients-3.2')">
                    <div class="flex-1">
                        <h3>Exercise 3.2: Gradient of Quadratic Form</h3>
                        <div class="content-text">
                            For f(x) = x·µÄAx where x = [x‚ÇÅ, x‚ÇÇ]·µÄ and A = [[2, 1], [1, 3]]
                            <ol class="list-decimal ml-6 my-2">
                                <li>Expand f(x) in terms of x‚ÇÅ, x‚ÇÇ</li>
                                <li>Calculate ‚àÇf/‚àÇx‚ÇÅ</li>
                                <li>Calculate ‚àÇf/‚àÇx‚ÇÇ</li>
                                <li>Write the gradient ‚àáf(x)</li>
                                <li>Verify the formula: ‚àáf(x) = (A + A·µÄ)x</li>
                            </ol>
                        </div>
                    </div>
                </label>
            </div>

            <div class="exercise-item" data-exercise-id="calculus-gradients-3.3">
                <label class="flex items-start space-x-3 cursor-pointer">
                    <input type="checkbox" class="checkbox-custom mt-1" onchange="toggleExercise('calculus-gradients-3.3')">
                    <div class="flex-1">
                        <h3>Exercise 3.3: Gradient Descent Step</h3>
                        <div class="content-text">
                            Given f(x, y) = x¬≤ + 4y¬≤ and starting point (x‚ÇÄ, y‚ÇÄ) = (4, 2):
                            <ol class="list-decimal ml-6 my-2">
                                <li>Calculate gradient at (4, 2)</li>
                                <li>Using learning rate Œ± = 0.1, calculate one gradient descent step: (x‚ÇÅ, y‚ÇÅ) = (x‚ÇÄ, y‚ÇÄ) - Œ±‚àáf(x‚ÇÄ, y‚ÇÄ)</li>
                                <li>Calculate f(x‚ÇÄ, y‚ÇÄ) and f(x‚ÇÅ, y‚ÇÅ) - did we reduce the function?</li>
                                <li>Calculate the gradient at the new point (x‚ÇÅ, y‚ÇÅ)</li>
                            </ol>
                        </div>
                    </div>
                </label>
            </div>

            <h2 class="section-header">Part 4: Chain Rule for Multivariable Functions (60 min)</h2>

            <div class="exercise-item" data-exercise-id="calculus-gradients-4.1">
                <label class="flex items-start space-x-3 cursor-pointer">
                    <input type="checkbox" class="checkbox-custom mt-1" onchange="toggleExercise('calculus-gradients-4.1')">
                    <div class="flex-1">
                        <h3>Exercise 4.1: Simple Chain Rule</h3>
                        <div class="content-text">
                            Let z = f(x, y) = x¬≤ + y¬≤ where x = 2t and y = 3t<br><br>
                            Find dz/dt using chain rule:<br>
                            dz/dt = (‚àÇf/‚àÇx)(dx/dt) + (‚àÇf/‚àÇy)(dy/dt)
                        </div>
                    </div>
                </label>
            </div>

            <div class="exercise-item" data-exercise-id="calculus-gradients-4.2">
                <label class="flex items-start space-x-3 cursor-pointer">
                    <input type="checkbox" class="checkbox-custom mt-1" onchange="toggleExercise('calculus-gradients-4.2')">
                    <div class="flex-1">
                        <h3>Exercise 4.2: Backpropagation Example</h3>
                        <div class="content-text">
                            Consider a simple neural network computation:
                            <pre>Input: x
Hidden: h = œÉ(wx + b)  where œÉ(z) = 1/(1 + e^(-z))
Output: y = h¬≤
Loss: L = (y - t)¬≤  where t is target</pre>
                            Given x = 2, w = 0.5, b = 1, t = 0.8:
                            <ol class="list-decimal ml-6 my-2">
                                <li><strong>Forward pass:</strong> Calculate h, y, L (show all steps)</li>
                                <li><strong>Backward pass:</strong> Calculate gradients using chain rule:
                                    <ul class="list-disc ml-6 my-1">
                                        <li>‚àÇL/‚àÇy</li>
                                        <li>‚àÇy/‚àÇh</li>
                                        <li>‚àÇh/‚àÇw</li>
                                        <li>‚àÇh/‚àÇb</li>
                                        <li>‚àÇL/‚àÇw = (‚àÇL/‚àÇy)(‚àÇy/‚àÇh)(‚àÇh/‚àÇw)</li>
                                        <li>‚àÇL/‚àÇb = (‚àÇL/‚àÇy)(‚àÇy/‚àÇh)(‚àÇh/‚àÇb)</li>
                                    </ul>
                                </li>
                            </ol>
                            Note: œÉ'(z) = œÉ(z)(1 - œÉ(z))
                        </div>
                    </div>
                </label>
            </div>

            <div class="exercise-item" data-exercise-id="calculus-gradients-4.3">
                <label class="flex items-start space-x-3 cursor-pointer">
                    <input type="checkbox" class="checkbox-custom mt-1" onchange="toggleExercise('calculus-gradients-4.3')">
                    <div class="flex-1">
                        <h3>Exercise 4.3: Vector Chain Rule</h3>
                        <div class="content-text">
                            Given:
                            <ul class="list-disc ml-6 my-2">
                                <li>z = f(y) = y‚ÇÅ¬≤ + y‚ÇÇ¬≤</li>
                                <li>y = g(x) = [2x‚ÇÅ + x‚ÇÇ, x‚ÇÅ - x‚ÇÇ]·µÄ</li>
                            </ul>
                            Find ‚àÇz/‚àÇx‚ÇÅ and ‚àÇz/‚àÇx‚ÇÇ using chain rule:<br>
                            ‚àÇz/‚àÇx·µ¢ = Œ£‚±º (‚àÇz/‚àÇy‚±º)(‚àÇy‚±º/‚àÇx·µ¢)
                        </div>
                    </div>
                </label>
            </div>

            <h2 class="section-header">Part 5: Jacobian Matrices (45 min)</h2>

            <div class="exercise-item" data-exercise-id="calculus-gradients-5.1">
                <label class="flex items-start space-x-3 cursor-pointer">
                    <input type="checkbox" class="checkbox-custom mt-1" onchange="toggleExercise('calculus-gradients-5.1')">
                    <div class="flex-1">
                        <h3>Exercise 5.1: Computing Jacobian</h3>
                        <div class="content-text">
                            For function f: ‚Ñù¬≤ ‚Üí ‚Ñù¬≥ defined by:
                            <ul class="list-disc ml-6 my-2">
                                <li>f‚ÇÅ(x‚ÇÅ, x‚ÇÇ) = x‚ÇÅ¬≤ + x‚ÇÇ</li>
                                <li>f‚ÇÇ(x‚ÇÅ, x‚ÇÇ) = x‚ÇÅx‚ÇÇ</li>
                                <li>f‚ÇÉ(x‚ÇÅ, x‚ÇÇ) = x‚ÇÅ + 2x‚ÇÇ¬≤</li>
                            </ul>
                            Calculate the Jacobian matrix:
                            <pre>J = [[‚àÇf‚ÇÅ/‚àÇx‚ÇÅ, ‚àÇf‚ÇÅ/‚àÇx‚ÇÇ],
     [‚àÇf‚ÇÇ/‚àÇx‚ÇÅ, ‚àÇf‚ÇÇ/‚àÇx‚ÇÇ],
     [‚àÇf‚ÇÉ/‚àÇx‚ÇÅ, ‚àÇf‚ÇÉ/‚àÇx‚ÇÇ]]</pre>
                            Evaluate at point (1, 2).
                        </div>
                    </div>
                </label>
            </div>

            <div class="exercise-item" data-exercise-id="calculus-gradients-5.2">
                <label class="flex items-start space-x-3 cursor-pointer">
                    <input type="checkbox" class="checkbox-custom mt-1" onchange="toggleExercise('calculus-gradients-5.2')">
                    <div class="flex-1">
                        <h3>Exercise 5.2: Chain Rule with Jacobians</h3>
                        <div class="content-text">
                            Given:
                            <ul class="list-disc ml-6 my-2">
                                <li>z = f(y): ‚Ñù¬≤ ‚Üí ‚Ñù where f(y‚ÇÅ, y‚ÇÇ) = y‚ÇÅ¬≤ + 2y‚ÇÇ¬≤</li>
                                <li>y = g(x): ‚Ñù¬≥ ‚Üí ‚Ñù¬≤ where g(x‚ÇÅ, x‚ÇÇ, x‚ÇÉ) = [x‚ÇÅ + x‚ÇÇ, x‚ÇÇx‚ÇÉ]·µÄ</li>
                            </ul>
                            <ol class="list-decimal ml-6">
                                <li>Calculate ‚àáf (gradient of f)</li>
                                <li>Calculate Jacobian of g: Jg (2√ó3 matrix)</li>
                                <li>Calculate gradient of z with respect to x using: ‚àá‚Çìz = Jg·µÄ‚àáf</li>
                            </ol>
                        </div>
                    </div>
                </label>
            </div>

            <h2 class="section-header">Part 6: Hessian Matrices (30 min)</h2>

            <div class="exercise-item" data-exercise-id="calculus-gradients-6.1">
                <label class="flex items-start space-x-3 cursor-pointer">
                    <input type="checkbox" class="checkbox-custom mt-1" onchange="toggleExercise('calculus-gradients-6.1')">
                    <div class="flex-1">
                        <h3>Exercise 6.1: Computing Hessian</h3>
                        <div class="content-text">
                            For f(x, y) = x¬≥ + y¬≥ - 3xy<br><br>
                            Calculate the Hessian matrix:
                            <pre>H = [[‚àÇ¬≤f/‚àÇx¬≤, ‚àÇ¬≤f/‚àÇx‚àÇy],
     [‚àÇ¬≤f/‚àÇy‚àÇx, ‚àÇ¬≤f/‚àÇy¬≤]]</pre>
                            Evaluate at point (1, 1).
                        </div>
                    </div>
                </label>
            </div>

            <div class="exercise-item" data-exercise-id="calculus-gradients-6.2">
                <label class="flex items-start space-x-3 cursor-pointer">
                    <input type="checkbox" class="checkbox-custom mt-1" onchange="toggleExercise('calculus-gradients-6.2')">
                    <div class="flex-1">
                        <h3>Exercise 6.2: Analyzing Critical Points</h3>
                        <div class="content-text">
                            For f(x, y) = x¬≤ - xy + y¬≤ + 2x - y
                            <ol class="list-decimal ml-6 my-2">
                                <li>Find critical points (solve ‚àáf = 0)</li>
                                <li>Calculate Hessian at each critical point</li>
                                <li>Determine nature of each critical point using second derivative test:
                                    <ul class="list-disc ml-6 my-1">
                                        <li>If det(H) > 0 and ‚àÇ¬≤f/‚àÇx¬≤ > 0: local minimum</li>
                                        <li>If det(H) > 0 and ‚àÇ¬≤f/‚àÇx¬≤ < 0: local maximum</li>
                                        <li>If det(H) < 0: saddle point</li>
                                    </ul>
                                </li>
                            </ol>
                        </div>
                    </div>
                </label>
            </div>

            <h2 class="section-header">Part 7: ML-Specific Gradients (60 min)</h2>

            <div class="exercise-item" data-exercise-id="calculus-gradients-7.1">
                <label class="flex items-start space-x-3 cursor-pointer">
                    <input type="checkbox" class="checkbox-custom mt-1" onchange="toggleExercise('calculus-gradients-7.1')">
                    <div class="flex-1">
                        <h3>Exercise 7.1: Linear Regression Gradient</h3>
                        <div class="content-text">
                            For linear regression: ≈∑ = wx + b<br>
                            Loss: L = (y - ≈∑)¬≤ = (y - wx - b)¬≤<br><br>
                            Given data point: x = 3, y = 7<br>
                            Current parameters: w = 1.5, b = 2
                            <ol class="list-decimal ml-6 my-2">
                                <li>Calculate predicted value ≈∑</li>
                                <li>Calculate loss L</li>
                                <li>Calculate ‚àÇL/‚àÇw (show chain rule steps)</li>
                                <li>Calculate ‚àÇL/‚àÇb</li>
                                <li>Update parameters using Œ± = 0.1:
                                    <ul class="list-disc ml-6 my-1">
                                        <li>w_new = w - Œ±(‚àÇL/‚àÇw)</li>
                                        <li>b_new = b - Œ±(‚àÇL/‚àÇb)</li>
                                    </ul>
                                </li>
                            </ol>
                        </div>
                    </div>
                </label>
            </div>

            <div class="exercise-item" data-exercise-id="calculus-gradients-7.2">
                <label class="flex items-start space-x-3 cursor-pointer">
                    <input type="checkbox" class="checkbox-custom mt-1" onchange="toggleExercise('calculus-gradients-7.2')">
                    <div class="flex-1">
                        <h3>Exercise 7.2: Logistic Regression Gradient</h3>
                        <div class="content-text">
                            For binary classification:
                            <ul class="list-disc ml-6 my-2">
                                <li>Prediction: ≈∑ = œÉ(wx + b) where œÉ(z) = 1/(1 + e^(-z))</li>
                                <li>Binary cross-entropy loss: L = -[y log(≈∑) + (1-y) log(1-≈∑)]</li>
                            </ul>
                            Given: x = 2, y = 1 (true class), w = 0.5, b = 0.5
                            <ol class="list-decimal ml-6 my-2">
                                <li>Calculate z = wx + b</li>
                                <li>Calculate ≈∑ = œÉ(z)</li>
                                <li>Calculate loss L</li>
                                <li>Calculate ‚àÇL/‚àÇ≈∑</li>
                                <li>Calculate ‚àÇ≈∑/‚àÇz (remember: œÉ'(z) = œÉ(z)(1-œÉ(z)))</li>
                                <li>Calculate ‚àÇz/‚àÇw and ‚àÇz/‚àÇb</li>
                                <li>Use chain rule: ‚àÇL/‚àÇw = (‚àÇL/‚àÇ≈∑)(‚àÇ≈∑/‚àÇz)(‚àÇz/‚àÇw)</li>
                                <li>Calculate ‚àÇL/‚àÇb similarly</li>
                            </ol>
                        </div>
                    </div>
                </label>
            </div>

            <div class="exercise-item" data-exercise-id="calculus-gradients-7.3">
                <label class="flex items-start space-x-3 3 cursor-pointer">
                    <input type="checkbox" class="checkbox-custom mt-1" onchange="toggleExercise('calculus-gradients-7.3')">
                    <div class="flex-1">
                        <h3>Exercise 7.3: Softmax Gradient (Challenging)</h3>
                        <div class="content-text">
                            For multi-class classification with 3 classes:
                            <pre>Logits: z = [z‚ÇÅ, z‚ÇÇ, z‚ÇÉ]·µÄ
Softmax: ≈∑·µ¢ = e^(z·µ¢) / Œ£‚±ºe^(z‚±º)
Cross-entropy: L = -Œ£·µ¢ y·µ¢ log(≈∑·µ¢)</pre>
                            Given: z = [2, 1, 0.5], y = [1, 0, 0] (one-hot encoded, class 0 is correct)
                            <ol class="list-decimal ml-6 my-2">
                                <li>Calculate softmax outputs ≈∑‚ÇÅ, ≈∑‚ÇÇ, ≈∑‚ÇÉ</li>
                                <li>Calculate loss L</li>
                                <li>Show that ‚àÇL/‚àÇz·µ¢ = ≈∑·µ¢ - y·µ¢ (this is a remarkable simplification!)</li>
                                <li>Calculate gradients ‚àÇL/‚àÇz‚ÇÅ, ‚àÇL/‚àÇz‚ÇÇ, ‚àÇL/‚àÇz‚ÇÉ</li>
                            </ol>
                        </div>
                    </div>
                </label>
            </div>

            <h2 class="section-header">Challenge Problems (Optional)</h2>

            <div class="exercise-item" data-exercise-id="calculus-gradients-challenge-1">
                <label class="flex items-start space-x-3 cursor-pointer">
                    <input type="checkbox" class="checkbox-custom mt-1" onchange="toggleExercise('calculus-gradients-challenge-1')">
                    <div class="flex-1">
                        <h3>Challenge 1: Newton's Method</h3>
                        <div class="content-text">
                            Implement one step of Newton's method for minimizing f(x, y) = x¬≤ + 4y¬≤<br><br>
                            Formula: x_new = x - H‚Åª¬π‚àáf<br><br>
                            Starting from (4, 2), calculate the next point.
                        </div>
                    </div>
                </label>
            </div>

            <div class="exercise-item" data-exercise-id="calculus-gradients-challenge-2">
                <label class="flex items-start space-x-3 cursor-pointer">
                    <input type="checkbox" class="checkbox-custom mt-1" onchange="toggleExercise('calculus-gradients-challenge-2')">
                    <div class="flex-1">
                        <h3>Challenge 2: Batch Gradient</h3>
                        <div class="content-text">
                            Extend Exercise 7.1 to mini-batch of 3 points:
                            <ul class="list-disc ml-6 my-2">
                                <li>(x‚ÇÅ, y‚ÇÅ) = (1, 3)</li>
                                <li>(x‚ÇÇ, y‚ÇÇ) = (2, 5)</li>
                                <li>(x‚ÇÉ, y‚ÇÉ) = (3, 7)</li>
                            </ul>
                            Calculate average gradient over the batch.
                        </div>
                    </div>
                </label>
            </div>

            <div class="exercise-item" data-exercise-id="calculus-gradients-challenge-3">
                <label class="flex items-start space-x-3 cursor-pointer">
                    <input type="checkbox" class="checkbox-custom mt-1" onchange="toggleExercise('calculus-gradients-challenge-3')">
                    <div class="flex-1">
                        <h3>Challenge 3: Derive Backprop for 2-Layer Network</h3>
                        <div class="content-text">
                            <pre>x ‚Üí h‚ÇÅ = œÉ(W‚ÇÅx + b‚ÇÅ) ‚Üí h‚ÇÇ = œÉ(W‚ÇÇh‚ÇÅ + b‚ÇÇ) ‚Üí L = (h‚ÇÇ - y)¬≤</pre>
                            Derive ‚àÇL/‚àÇW‚ÇÅ, ‚àÇL/‚àÇb‚ÇÅ, ‚àÇL/‚àÇW‚ÇÇ, ‚àÇL/‚àÇb‚ÇÇ using chain rule.
                        </div>
                    </div>
                </label>
            </div>

            <div class="mt-8 p-4 bg-gray-50 rounded-lg">
                <h3 class="font-semibold text-gray-800 mb-2">Tips for Success</h3>
                <ul class="list-disc ml-6 text-sm text-gray-700 space-y-1">
                    <li><strong>Show your work</strong> - Write out each step of the chain rule</li>
                    <li><strong>Verify numerically</strong> - Use numerical gradients to check your answers</li>
                    <li><strong>Draw computation graphs</strong> - Visualize the forward and backward pass</li>
                    <li><strong>Practice chain rule</strong> - It's the foundation of backpropagation</li>
                    <li><strong>Understand shapes</strong> - Keep track of matrix/vector dimensions</li>
                </ul>
            </div>
        </div>

        <!-- Footer -->
        <div class="mt-6 text-center text-gray-600 text-sm space-x-4">
            <a href="../solutions/calculus_gradients_solutions.html" class="text-emerald-600 hover:text-emerald-700 font-semibold">
                View Solutions ‚Üí
            </a>
            <span>|</span>
            <a href="../../index.html" class="text-indigo-600 hover:text-indigo-700 font-semibold">
                ‚Üê Back to ML Roadmap
            </a>
        </div>
    </div>

    <script>
        // Load completion state from localStorage
        function loadCompletionState() {
            const state = JSON.parse(localStorage.getItem('exercise-completions') || '{}');
            let completed = 0;
            let total = 0;

            document.querySelectorAll('.exercise-item').forEach(item => {
                const id = item.getAttribute('data-exercise-id');
                const checkbox = item.querySelector('input[type="checkbox"]');
                total++;

                if (state[id]) {
                    checkbox.checked = true;
                    item.classList.add('completed');
                    item.querySelectorAll('.content-text').forEach(el => el.classList.add('completed'));
                    completed++;
                }
            });

            updateProgress(completed, total);
        }

        // Toggle exercise completion
        function toggleExercise(exerciseId) {
            const state = JSON.parse(localStorage.getItem('exercise-completions') || '{}');
            state[exerciseId] = !state[exerciseId];

            if (!state[exerciseId]) {
                delete state[exerciseId];
            }

            localStorage.setItem('exercise-completions', JSON.stringify(state));

            // Update UI
            const item = document.querySelector(`[data-exercise-id="${exerciseId}"]`);
            if (state[exerciseId]) {
                item.classList.add('completed');
                item.querySelectorAll('.content-text').forEach(el => el.classList.add('completed'));
            } else {
                item.classList.remove('completed');
                item.querySelectorAll('.content-text').forEach(el => el.classList.remove('completed'));
            }

            // Recalculate progress
            loadCompletionState();
        }

        // Update progress bar
        function updateProgress(completed, total) {
            const percentage = total > 0 ? Math.round((completed / total) * 100) : 0;
            document.getElementById('progress-bar').style.width = percentage + '%';
            document.getElementById('progress-text').textContent = percentage + '%';
        }

        // Initialize on page load
        document.addEventListener('DOMContentLoaded', loadCompletionState);
    </script>
</body>
</html>
