# Matrix Calculus & Gradients - Solutions (Module 2)

**Time:** Reference for 3-4 hours of exercises
**Difficulty:** Intermediate

Complete solutions to exercises in `guides/exercises/calculus_gradients_exercises.md`

---

## Part 1: Scalar Derivatives Review - Solutions

### Exercise 1.1: Basic Derivatives

**1. f(x) = 3xÂ² + 2x - 5**

f'(x) = d/dx(3xÂ²) + d/dx(2x) - d/dx(5)
     = 3(2x) + 2 - 0
     = **6x + 2**

**2. f(x) = xÂ³ - 4xÂ² + x**

f'(x) = d/dx(xÂ³) - d/dx(4xÂ²) + d/dx(x)
     = 3xÂ² - 4(2x) + 1
     = **3xÂ² - 8x + 1**

**3. f(x) = 1/xÂ² = xâ»Â²**

f'(x) = d/dx(xâ»Â²)
     = -2xâ»Â³
     = **-2/xÂ³**

**4. f(x) = e^(2x)**

Using chain rule:
f'(x) = e^(2x) Â· d/dx(2x)
     = e^(2x) Â· 2
     = **2e^(2x)**

**5. f(x) = ln(xÂ²)**

Using chain rule:
f'(x) = (1/xÂ²) Â· d/dx(xÂ²)
     = (1/xÂ²) Â· 2x
     = **2/x**

Alternatively: ln(xÂ²) = 2ln(x), so f'(x) = 2/x âœ“

---

### Exercise 1.2: Chain Rule

**1. f(x) = (3x + 2)â´**

Let u = 3x + 2, then f = uâ´

f'(x) = d/du(uâ´) Â· du/dx
     = 4uÂ³ Â· 3
     = **12(3x + 2)Â³**

**2. f(x) = e^(xÂ²)**

Let u = xÂ², then f = e^u

f'(x) = e^u Â· du/dx
     = e^(xÂ²) Â· 2x
     = **2x Â· e^(xÂ²)**

**3. f(x) = ln(2x + 1)**

Let u = 2x + 1, then f = ln(u)

f'(x) = (1/u) Â· du/dx
     = 1/(2x + 1) Â· 2
     = **2/(2x + 1)**

**4. f(x) = sin(3xÂ²)**

Let u = 3xÂ², then f = sin(u)

f'(x) = cos(u) Â· du/dx
     = cos(3xÂ²) Â· 6x
     = **6x Â· cos(3xÂ²)**

---

### Exercise 1.3: Product and Quotient Rules

**1. f(x) = xÂ² Â· e^x** (Product rule)

u = xÂ², u' = 2x
v = e^x, v' = e^x

f'(x) = u'v + uv'
     = 2x Â· e^x + xÂ² Â· e^x
     = **e^x(2x + xÂ²)**
     = **e^x Â· x(x + 2)**

**2. f(x) = xÂ³ Â· ln(x)** (Product rule)

u = xÂ³, u' = 3xÂ²
v = ln(x), v' = 1/x

f'(x) = u'v + uv'
     = 3xÂ² Â· ln(x) + xÂ³ Â· (1/x)
     = 3xÂ² ln(x) + xÂ²
     = **xÂ²(3ln(x) + 1)**

**3. f(x) = (xÂ² + 1)/(x - 1)** (Quotient rule)

u = xÂ² + 1, u' = 2x
v = x - 1, v' = 1

f'(x) = (u'v - uv')/vÂ²
     = [2x(x - 1) - (xÂ² + 1)(1)]/(x - 1)Â²
     = [2xÂ² - 2x - xÂ² - 1]/(x - 1)Â²
     = **(xÂ² - 2x - 1)/(x - 1)Â²**

---

## Part 2: Partial Derivatives - Solutions

### Exercise 2.1: Basic Partial Derivatives

**f(x, y) = xÂ²y + 3xyÂ² - 2x + y**

**1. âˆ‚f/âˆ‚x** (treat y as constant):
âˆ‚f/âˆ‚x = 2xy + 3yÂ² - 2

**2. âˆ‚f/âˆ‚y** (treat x as constant):
âˆ‚f/âˆ‚y = xÂ² + 6xy + 1

**3. Evaluate at (1, 2):**

âˆ‚f/âˆ‚x|(1,2) = 2(1)(2) + 3(2)Â² - 2
            = 4 + 12 - 2
            = **14**

âˆ‚f/âˆ‚y|(1,2) = (1)Â² + 6(1)(2) + 1
            = 1 + 12 + 1
            = **14**

---

### Exercise 2.2: More Partial Derivatives

**f(x, y) = e^(xy) + xÂ²yÂ³**

**1. âˆ‚f/âˆ‚x:**
âˆ‚f/âˆ‚x = e^(xy) Â· y + 2xyÂ³
     = **ye^(xy) + 2xyÂ³**

**2. âˆ‚f/âˆ‚y:**
âˆ‚f/âˆ‚y = e^(xy) Â· x + 3xÂ²yÂ²
     = **xe^(xy) + 3xÂ²yÂ²**

**3. Evaluate at (0, 1):**

âˆ‚f/âˆ‚x|(0,1) = 1 Â· e^(0Â·1) + 2(0)(1)Â³
            = 1 Â· 1 + 0
            = **1**

âˆ‚f/âˆ‚y|(0,1) = 0 Â· e^(0Â·1) + 3(0)Â²(1)Â²
            = 0 + 0
            = **0**

---

### Exercise 2.3: Second-Order Partial Derivatives

**f(x, y) = xÂ³yÂ² - 2xy + 5**

**First-order partials:**
âˆ‚f/âˆ‚x = 3xÂ²yÂ² - 2y
âˆ‚f/âˆ‚y = 2xÂ³y - 2x

**1. âˆ‚Â²f/âˆ‚xÂ²:**
âˆ‚Â²f/âˆ‚xÂ² = âˆ‚/âˆ‚x(3xÂ²yÂ² - 2y)
        = **6xyÂ²**

**2. âˆ‚Â²f/âˆ‚yÂ²:**
âˆ‚Â²f/âˆ‚yÂ² = âˆ‚/âˆ‚y(2xÂ³y - 2x)
        = **2xÂ³**

**3. âˆ‚Â²f/âˆ‚xâˆ‚y** (differentiate âˆ‚f/âˆ‚x with respect to y):
âˆ‚Â²f/âˆ‚xâˆ‚y = âˆ‚/âˆ‚y(3xÂ²yÂ² - 2y)
          = 6xÂ²y - 2
          = **6xÂ²y - 2**

**4. âˆ‚Â²f/âˆ‚yâˆ‚x** (differentiate âˆ‚f/âˆ‚y with respect to x):
âˆ‚Â²f/âˆ‚yâˆ‚x = âˆ‚/âˆ‚x(2xÂ³y - 2x)
          = 6xÂ²y - 2
          = **6xÂ²y - 2**

**5. Verification:**
âˆ‚Â²f/âˆ‚xâˆ‚y = âˆ‚Â²f/âˆ‚yâˆ‚x = 6xÂ²y - 2 âœ“

This confirms Clairaut's theorem (mixed partials are equal for continuous functions).

---

## Part 3: Gradients - Solutions

### Exercise 3.1: Computing Gradients

**f(x, y) = xÂ² + yÂ² - 2x - 4y + 5**

**1. Calculate gradient:**

âˆ‚f/âˆ‚x = 2x - 2
âˆ‚f/âˆ‚y = 2y - 4

**âˆ‡f = [2x - 2, 2y - 4]áµ€**

**2. Gradient at (1, 2):**

âˆ‡f(1, 2) = [2(1) - 2, 2(2) - 4]
         = **[0, 0]áµ€**

**3. Critical points (where âˆ‡f = 0):**

2x - 2 = 0  âŸ¹  x = 1
2y - 4 = 0  âŸ¹  y = 2

**Critical point: (1, 2)**

**4. Classify critical point:**

Hessian matrix:
H = [âˆ‚Â²f/âˆ‚xÂ²    âˆ‚Â²f/âˆ‚xâˆ‚y]   [2  0]
    [âˆ‚Â²f/âˆ‚yâˆ‚x   âˆ‚Â²f/âˆ‚yÂ²  ] = [0  2]

Both eigenvalues are 2 > 0 (or simply: det(H) = 4 > 0 and âˆ‚Â²f/âˆ‚xÂ² = 2 > 0)

**Answer: Local minimum at (1, 2)**

Function value: f(1, 2) = 1 + 4 - 2 - 8 + 5 = 0

---

### Exercise 3.2: Gradient of Quadratic Form

**f(x) = xáµ€Ax where x = [xâ‚, xâ‚‚]áµ€ and A = [[2, 1], [1, 3]]**

**1. Expand f(x):**

f(x) = [xâ‚ xâ‚‚] [2  1] [xâ‚]
                [1  3] [xâ‚‚]

     = [xâ‚ xâ‚‚] [2xâ‚ + xâ‚‚ ]
                [xâ‚ + 3xâ‚‚]

     = xâ‚(2xâ‚ + xâ‚‚) + xâ‚‚(xâ‚ + 3xâ‚‚)
     = 2xâ‚Â² + xâ‚xâ‚‚ + xâ‚xâ‚‚ + 3xâ‚‚Â²
     = **2xâ‚Â² + 2xâ‚xâ‚‚ + 3xâ‚‚Â²**

**2. âˆ‚f/âˆ‚xâ‚:**
âˆ‚f/âˆ‚xâ‚ = 4xâ‚ + 2xâ‚‚

**3. âˆ‚f/âˆ‚xâ‚‚:**
âˆ‚f/âˆ‚xâ‚‚ = 2xâ‚ + 6xâ‚‚

**4. Gradient:**
**âˆ‡f(x) = [4xâ‚ + 2xâ‚‚, 2xâ‚ + 6xâ‚‚]áµ€**

**5. Verify formula âˆ‡f(x) = (A + Aáµ€)x:**

A + Aáµ€ = [2  1] + [2  1] = [4  2]
         [1  3]   [1  3]   [2  6]

(A + Aáµ€)x = [4  2] [xâ‚] = [4xâ‚ + 2xâ‚‚]
            [2  6] [xâ‚‚]   [2xâ‚ + 6xâ‚‚]

**Verified! âœ“**

Note: For symmetric matrices (A = Aáµ€), this simplifies to âˆ‡f(x) = 2Ax

---

### Exercise 3.3: Gradient Descent Step

**f(x, y) = xÂ² + 4yÂ²**
**Starting point: (xâ‚€, yâ‚€) = (4, 2)**
**Learning rate: Î± = 0.1**

**1. Gradient at (4, 2):**

âˆ‚f/âˆ‚x = 2x
âˆ‚f/âˆ‚y = 8y

âˆ‡f(4, 2) = [2(4), 8(2)]
         = **[8, 16]áµ€**

**2. Gradient descent update:**

[xâ‚]   [xâ‚€]       [âˆ‚f/âˆ‚x]
[yâ‚] = [yâ‚€] - Î± Â· [âˆ‚f/âˆ‚y]

     = [4] - 0.1 Â· [8 ]
       [2]         [16]

     = [4] - [0.8]
       [2]   [1.6]

     = **[3.2, 0.4]áµ€**

**3. Function values:**

f(4, 2) = (4)Â² + 4(2)Â² = 16 + 16 = **32**

f(3.2, 0.4) = (3.2)Â² + 4(0.4)Â² = 10.24 + 0.64 = **10.88**

**Yes, function decreased from 32 to 10.88! âœ“**

**4. Gradient at new point (3.2, 0.4):**

âˆ‡f(3.2, 0.4) = [2(3.2), 8(0.4)]
             = **[6.4, 3.2]áµ€**

Gradient magnitude decreased from âˆš(8Â² + 16Â²) = 17.89 to âˆš(6.4Â² + 3.2Â²) = 7.16

---

## Part 4: Chain Rule for Multivariable Functions - Solutions

### Exercise 4.1: Simple Chain Rule

**z = f(x, y) = xÂ² + yÂ²**
**x = 2t, y = 3t**

**Find dz/dt:**

dz/dt = (âˆ‚f/âˆ‚x)(dx/dt) + (âˆ‚f/âˆ‚y)(dy/dt)

âˆ‚f/âˆ‚x = 2x
âˆ‚f/âˆ‚y = 2y
dx/dt = 2
dy/dt = 3

dz/dt = 2x Â· 2 + 2y Â· 3
      = 4x + 6y
      = 4(2t) + 6(3t)
      = 8t + 18t
      = **26t**

**Verification:** z = (2t)Â² + (3t)Â² = 4tÂ² + 9tÂ² = 13tÂ²
So dz/dt = 26t âœ“

---

### Exercise 4.2: Backpropagation Example

**Network:**
```
x â†’ h = Ïƒ(wx + b)  where Ïƒ(z) = 1/(1 + e^(-z))
h â†’ y = hÂ²
y â†’ L = (y - t)Â²  where t is target
```

**Given:** x = 2, w = 0.5, b = 1, t = 0.8

**1. Forward pass:**

**z = wx + b**
z = 0.5(2) + 1 = 2

**h = Ïƒ(z) = 1/(1 + e^(-z))**
h = 1/(1 + e^(-2))
h = 1/(1 + 0.1353)
h â‰ˆ **0.8808**

**y = hÂ²**
y = (0.8808)Â²
y â‰ˆ **0.7758**

**L = (y - t)Â²**
L = (0.7758 - 0.8)Â²
L = (-0.0242)Â²
L â‰ˆ **0.000586**

**2. Backward pass:**

**âˆ‚L/âˆ‚y:**
âˆ‚L/âˆ‚y = 2(y - t)
      = 2(0.7758 - 0.8)
      = 2(-0.0242)
      = **-0.0484**

**âˆ‚y/âˆ‚h:**
âˆ‚y/âˆ‚h = 2h
      = 2(0.8808)
      = **1.7616**

**âˆ‚h/âˆ‚w:**
âˆ‚h/âˆ‚z = Ïƒ'(z) = Ïƒ(z)(1 - Ïƒ(z))
      = 0.8808(1 - 0.8808)
      = 0.8808 Ã— 0.1192
      = **0.1050**

âˆ‚z/âˆ‚w = x = 2

âˆ‚h/âˆ‚w = (âˆ‚h/âˆ‚z)(âˆ‚z/âˆ‚w)
      = 0.1050 Ã— 2
      = **0.2100**

**âˆ‚h/âˆ‚b:**
âˆ‚z/âˆ‚b = 1

âˆ‚h/âˆ‚b = (âˆ‚h/âˆ‚z)(âˆ‚z/âˆ‚b)
      = 0.1050 Ã— 1
      = **0.1050**

**Chain rule - âˆ‚L/âˆ‚w:**
âˆ‚L/âˆ‚w = (âˆ‚L/âˆ‚y)(âˆ‚y/âˆ‚h)(âˆ‚h/âˆ‚w)
      = (-0.0484)(1.7616)(0.2100)
      = **-0.0179**

**Chain rule - âˆ‚L/âˆ‚b:**
âˆ‚L/âˆ‚b = (âˆ‚L/âˆ‚y)(âˆ‚y/âˆ‚h)(âˆ‚h/âˆ‚b)
      = (-0.0484)(1.7616)(0.1050)
      = **-0.0090**

**Interpretation:** Both gradients are negative, so to reduce loss, we should *increase* both w and b.

---

### Exercise 4.3: Vector Chain Rule

**Given:**
- z = f(y) = yâ‚Â² + yâ‚‚Â²
- y = g(x) = [2xâ‚ + xâ‚‚, xâ‚ - xâ‚‚]áµ€

**Find âˆ‚z/âˆ‚xâ‚ and âˆ‚z/âˆ‚xâ‚‚:**

**Chain rule:** âˆ‚z/âˆ‚xáµ¢ = Î£â±¼ (âˆ‚z/âˆ‚yâ±¼)(âˆ‚yâ±¼/âˆ‚xáµ¢)

**Compute gradients:**

âˆ‚z/âˆ‚yâ‚ = 2yâ‚
âˆ‚z/âˆ‚yâ‚‚ = 2yâ‚‚

âˆ‚yâ‚/âˆ‚xâ‚ = 2,  âˆ‚yâ‚/âˆ‚xâ‚‚ = 1
âˆ‚yâ‚‚/âˆ‚xâ‚ = 1,  âˆ‚yâ‚‚/âˆ‚xâ‚‚ = -1

**âˆ‚z/âˆ‚xâ‚:**
âˆ‚z/âˆ‚xâ‚ = (âˆ‚z/âˆ‚yâ‚)(âˆ‚yâ‚/âˆ‚xâ‚) + (âˆ‚z/âˆ‚yâ‚‚)(âˆ‚yâ‚‚/âˆ‚xâ‚)
       = 2yâ‚ Â· 2 + 2yâ‚‚ Â· 1
       = 4yâ‚ + 2yâ‚‚
       = 4(2xâ‚ + xâ‚‚) + 2(xâ‚ - xâ‚‚)
       = 8xâ‚ + 4xâ‚‚ + 2xâ‚ - 2xâ‚‚
       = **10xâ‚ + 2xâ‚‚**

**âˆ‚z/âˆ‚xâ‚‚:**
âˆ‚z/âˆ‚xâ‚‚ = (âˆ‚z/âˆ‚yâ‚)(âˆ‚yâ‚/âˆ‚xâ‚‚) + (âˆ‚z/âˆ‚yâ‚‚)(âˆ‚yâ‚‚/âˆ‚xâ‚‚)
       = 2yâ‚ Â· 1 + 2yâ‚‚ Â· (-1)
       = 2yâ‚ - 2yâ‚‚
       = 2(2xâ‚ + xâ‚‚) - 2(xâ‚ - xâ‚‚)
       = 4xâ‚ + 2xâ‚‚ - 2xâ‚ + 2xâ‚‚
       = **2xâ‚ + 4xâ‚‚**

---

## Part 5: Jacobian Matrices - Solutions

### Exercise 5.1: Computing Jacobian

**f: â„Â² â†’ â„Â³ defined by:**
```
fâ‚(xâ‚, xâ‚‚) = xâ‚Â² + xâ‚‚
fâ‚‚(xâ‚, xâ‚‚) = xâ‚xâ‚‚
fâ‚ƒ(xâ‚, xâ‚‚) = xâ‚ + 2xâ‚‚Â²
```

**Jacobian matrix:**

J = [âˆ‚fâ‚/âˆ‚xâ‚  âˆ‚fâ‚/âˆ‚xâ‚‚]   [2xâ‚    1  ]
    [âˆ‚fâ‚‚/âˆ‚xâ‚  âˆ‚fâ‚‚/âˆ‚xâ‚‚] = [xâ‚‚     xâ‚ ]
    [âˆ‚fâ‚ƒ/âˆ‚xâ‚  âˆ‚fâ‚ƒ/âˆ‚xâ‚‚]   [1      4xâ‚‚]

**Evaluate at (1, 2):**

**J(1, 2) = [2(1)   1  ]   [2  1]**
          **[2      1  ] = [2  1]**
          **[1    4(2)]   [1  8]**

---

### Exercise 5.2: Chain Rule with Jacobians

**Given:**
- z = f(y): â„Â² â†’ â„ where f(yâ‚, yâ‚‚) = yâ‚Â² + 2yâ‚‚Â²
- y = g(x): â„Â³ â†’ â„Â² where g(xâ‚, xâ‚‚, xâ‚ƒ) = [xâ‚ + xâ‚‚, xâ‚‚xâ‚ƒ]áµ€

**1. Calculate âˆ‡f (gradient of f):**

âˆ‚f/âˆ‚yâ‚ = 2yâ‚
âˆ‚f/âˆ‚yâ‚‚ = 4yâ‚‚

**âˆ‡f = [2yâ‚, 4yâ‚‚]áµ€** (2Ã—1 vector)

**2. Calculate Jacobian of g:**

yâ‚ = xâ‚ + xâ‚‚
yâ‚‚ = xâ‚‚xâ‚ƒ

Jg = [âˆ‚yâ‚/âˆ‚xâ‚  âˆ‚yâ‚/âˆ‚xâ‚‚  âˆ‚yâ‚/âˆ‚xâ‚ƒ]   [1   1   0 ]
     [âˆ‚yâ‚‚/âˆ‚xâ‚  âˆ‚yâ‚‚/âˆ‚xâ‚‚  âˆ‚yâ‚‚/âˆ‚xâ‚ƒ] = [0   xâ‚ƒ  xâ‚‚]

**Jg is 2Ã—3 matrix**

**3. Calculate âˆ‡â‚“z = Jgáµ€âˆ‡f:**

Jgáµ€ = [1    0 ]
      [1    xâ‚ƒ]
      [0    xâ‚‚]

âˆ‡â‚“z = [1    0 ] [2yâ‚]   [2yâ‚        ]
      [1    xâ‚ƒ] [4yâ‚‚] = [2yâ‚ + 4xâ‚ƒyâ‚‚]
      [0    xâ‚‚]         [4xâ‚‚yâ‚‚      ]

Substituting yâ‚ = xâ‚ + xâ‚‚ and yâ‚‚ = xâ‚‚xâ‚ƒ:

**âˆ‡â‚“z = [2(xâ‚ + xâ‚‚), 2(xâ‚ + xâ‚‚) + 4xâ‚ƒ(xâ‚‚xâ‚ƒ), 4xâ‚‚(xâ‚‚xâ‚ƒ)]áµ€**
     **= [2xâ‚ + 2xâ‚‚, 2xâ‚ + 2xâ‚‚ + 4xâ‚‚xâ‚ƒÂ², 4xâ‚‚Â²xâ‚ƒ]áµ€**

---

## Part 6: Hessian Matrices - Solutions

### Exercise 6.1: Computing Hessian

**f(x, y) = xÂ³ + yÂ³ - 3xy**

**First-order partials:**
âˆ‚f/âˆ‚x = 3xÂ² - 3y
âˆ‚f/âˆ‚y = 3yÂ² - 3x

**Second-order partials:**
âˆ‚Â²f/âˆ‚xÂ² = 6x
âˆ‚Â²f/âˆ‚yÂ² = 6y
âˆ‚Â²f/âˆ‚xâˆ‚y = -3
âˆ‚Â²f/âˆ‚yâˆ‚x = -3

**Hessian matrix:**

**H = [6x   -3]**
    **[-3   6y]**

**Evaluate at (1, 1):**

**H(1, 1) = [6(1)  -3]   [6  -3]**
          **[-3    6(1)] = [-3  6]**

---

### Exercise 6.2: Analyzing Critical Points

**f(x, y) = xÂ² - xy + yÂ² + 2x - y**

**1. Find critical points:**

âˆ‚f/âˆ‚x = 2x - y + 2 = 0
âˆ‚f/âˆ‚y = -x + 2y - 1 = 0

From second equation: x = 2y - 1

Substitute into first:
2(2y - 1) - y + 2 = 0
4y - 2 - y + 2 = 0
3y = 0
y = 0

Then: x = 2(0) - 1 = -1

**Critical point: (-1, 0)**

**2. Calculate Hessian:**

âˆ‚Â²f/âˆ‚xÂ² = 2
âˆ‚Â²f/âˆ‚yÂ² = 2
âˆ‚Â²f/âˆ‚xâˆ‚y = -1
âˆ‚Â²f/âˆ‚yâˆ‚x = -1

**H = [2   -1]**
    **[-1   2]**

**3. Classify critical point:**

det(H) = (2)(2) - (-1)(-1) = 4 - 1 = 3 > 0
âˆ‚Â²f/âˆ‚xÂ² = 2 > 0

Since det(H) > 0 and âˆ‚Â²f/âˆ‚xÂ² > 0:

**Answer: Local minimum at (-1, 0)**

Function value: f(-1, 0) = 1 - 0 + 0 - 2 - 0 = -1

---

## Part 7: ML-Specific Gradients - Solutions

### Exercise 7.1: Linear Regression Gradient

**Model:** Å· = wx + b
**Loss:** L = (y - Å·)Â²
**Data:** x = 3, y = 7
**Parameters:** w = 1.5, b = 2
**Learning rate:** Î± = 0.1

**1. Calculate predicted value:**
Å· = wx + b = 1.5(3) + 2 = 4.5 + 2 = **6.5**

**2. Calculate loss:**
L = (y - Å·)Â² = (7 - 6.5)Â² = (0.5)Â² = **0.25**

**3. Calculate âˆ‚L/âˆ‚w:**

L = (y - wx - b)Â²

Let u = y - wx - b, then L = uÂ²

âˆ‚L/âˆ‚u = 2u
âˆ‚u/âˆ‚w = -x

**âˆ‚L/âˆ‚w = (âˆ‚L/âˆ‚u)(âˆ‚u/âˆ‚w)**
       = 2(y - wx - b)(-x)
       = -2x(y - wx - b)

At current values:
âˆ‚L/âˆ‚w = -2(3)(7 - 1.5(3) - 2)
      = -6(7 - 4.5 - 2)
      = -6(0.5)
      = **-3**

**4. Calculate âˆ‚L/âˆ‚b:**

âˆ‚u/âˆ‚b = -1

**âˆ‚L/âˆ‚b = (âˆ‚L/âˆ‚u)(âˆ‚u/âˆ‚b)**
       = 2(y - wx - b)(-1)
       = -2(y - wx - b)

At current values:
âˆ‚L/âˆ‚b = -2(0.5)
      = **-1**

**5. Update parameters:**

w_new = w - Î±(âˆ‚L/âˆ‚w)
      = 1.5 - 0.1(-3)
      = 1.5 + 0.3
      = **1.8**

b_new = b - Î±(âˆ‚L/âˆ‚b)
      = 2 - 0.1(-1)
      = 2 + 0.1
      = **2.1**

**Verification:** New prediction: Å· = 1.8(3) + 2.1 = 5.4 + 2.1 = 7.5 (closer to target 7!) âœ“

---

### Exercise 7.2: Logistic Regression Gradient

**Model:** Å· = Ïƒ(z) where z = wx + b and Ïƒ(z) = 1/(1 + e^(-z))
**Loss:** L = -[y log(Å·) + (1-y) log(1-Å·)]
**Given:** x = 2, y = 1, w = 0.5, b = 0.5

**1. Calculate z:**
z = wx + b = 0.5(2) + 0.5 = 1 + 0.5 = **1.5**

**2. Calculate Å·:**
Å· = Ïƒ(1.5) = 1/(1 + e^(-1.5))
  = 1/(1 + 0.2231)
  â‰ˆ **0.8176**

**3. Calculate loss:**
L = -[y log(Å·) + (1-y) log(1-Å·)]
  = -[1 Â· log(0.8176) + 0 Â· log(0.1824)]
  = -log(0.8176)
  â‰ˆ **0.2014**

**4. Calculate âˆ‚L/âˆ‚Å·:**
âˆ‚L/âˆ‚Å· = -[y/Å· - (1-y)/(1-Å·)]
      = -[1/0.8176 - 0/0.1824]
      = -1.2231
      â‰ˆ **-1.223**

**5. Calculate âˆ‚Å·/âˆ‚z:**
Ïƒ'(z) = Ïƒ(z)(1 - Ïƒ(z))
      = 0.8176(1 - 0.8176)
      = 0.8176 Ã— 0.1824
      â‰ˆ **0.1491**

**6. Calculate âˆ‚z/âˆ‚w and âˆ‚z/âˆ‚b:**
âˆ‚z/âˆ‚w = x = **2**
âˆ‚z/âˆ‚b = **1**

**7. Chain rule - âˆ‚L/âˆ‚w:**
âˆ‚L/âˆ‚w = (âˆ‚L/âˆ‚Å·)(âˆ‚Å·/âˆ‚z)(âˆ‚z/âˆ‚w)
      = (-1.223)(0.1491)(2)
      â‰ˆ **-0.3648**

**8. Chain rule - âˆ‚L/âˆ‚b:**
âˆ‚L/âˆ‚b = (âˆ‚L/âˆ‚Å·)(âˆ‚Å·/âˆ‚z)(âˆ‚z/âˆ‚b)
      = (-1.223)(0.1491)(1)
      â‰ˆ **-0.1824**

**Simplified form (bonus):**
âˆ‚L/âˆ‚w = (Å· - y) Â· x = (0.8176 - 1) Â· 2 = -0.3648 âœ“
âˆ‚L/âˆ‚b = (Å· - y) = 0.8176 - 1 = -0.1824 âœ“

---

### Exercise 7.3: Softmax Gradient

**Given:** z = [2, 1, 0.5], y = [1, 0, 0] (class 0 is correct)

**1. Calculate softmax outputs:**

e^(zâ‚) = e^2 â‰ˆ 7.389
e^(zâ‚‚) = e^1 â‰ˆ 2.718
e^(zâ‚ƒ) = e^0.5 â‰ˆ 1.649

Sum = 7.389 + 2.718 + 1.649 = 11.756

Å·â‚ = 7.389/11.756 â‰ˆ **0.6285**
Å·â‚‚ = 2.718/11.756 â‰ˆ **0.2312**
Å·â‚ƒ = 1.649/11.756 â‰ˆ **0.1403**

Verification: 0.6285 + 0.2312 + 0.1403 = 1.0000 âœ“

**2. Calculate loss:**
L = -Î£áµ¢ yáµ¢ log(Å·áµ¢)
  = -(1 Â· log(0.6285) + 0 Â· log(0.2312) + 0 Â· log(0.1403))
  = -log(0.6285)
  â‰ˆ **0.4644**

**3. Show that âˆ‚L/âˆ‚záµ¢ = Å·áµ¢ - yáµ¢:**

For softmax + cross-entropy, the gradient simplifies beautifully:

âˆ‚L/âˆ‚záµ¢ = Å·áµ¢ - yáµ¢

This is a well-known result! The derivation involves:
- âˆ‚L/âˆ‚Å·áµ¢ = -yáµ¢/Å·áµ¢
- âˆ‚Å·áµ¢/âˆ‚zâ±¼ = Å·áµ¢(Î´áµ¢â±¼ - Å·â±¼) where Î´áµ¢â±¼ is Kronecker delta
- Chain rule over all outputs

The terms magically cancel to give the simple form!

**4. Calculate gradients:**

âˆ‚L/âˆ‚zâ‚ = Å·â‚ - yâ‚ = 0.6285 - 1 = **-0.3715**
âˆ‚L/âˆ‚zâ‚‚ = Å·â‚‚ - yâ‚‚ = 0.2312 - 0 = **0.2312**
âˆ‚L/âˆ‚zâ‚ƒ = Å·â‚ƒ - yâ‚ƒ = 0.1403 - 0 = **0.1403**

**Interpretation:**
- Class 0 gradient is negative (we want to increase logit for correct class)
- Classes 1, 2 gradients are positive (we want to decrease logits for wrong classes)

---

## NumPy Verification

```python
import numpy as np

# Verify Exercise 4.1: Chain rule
def z_func(t):
    x = 2*t
    y = 3*t
    return x**2 + y**2

t = 1.0
h = 1e-7
numerical = (z_func(t + h) - z_func(t - h)) / (2*h)
analytical = 26 * t
print(f"Ex 4.1 - Numerical: {numerical:.6f}, Analytical: {analytical:.6f}")
# Output: 26.000000, 26.000000 âœ“

# Verify Exercise 7.3: Softmax gradient
z = np.array([2.0, 1.0, 0.5])
y = np.array([1, 0, 0])

# Forward pass
exp_z = np.exp(z)
softmax = exp_z / np.sum(exp_z)
loss = -np.sum(y * np.log(softmax))

# Gradient
grad = softmax - y

print(f"Ex 7.3 - Softmax: {softmax}")
print(f"Ex 7.3 - Loss: {loss:.4f}")
print(f"Ex 7.3 - Gradient: {grad}")
# Output matches our hand calculations âœ“
```

---

## Challenge Problems - Solutions

### Challenge 1: Newton's Method

**f(x, y) = xÂ² + 4yÂ²**
**Starting point:** (4, 2)

**Formula:** x_new = x - Hâ»Â¹âˆ‡f

**Gradient:**
âˆ‡f = [2x, 8y]áµ€

At (4, 2):
âˆ‡f(4, 2) = [8, 16]áµ€

**Hessian:**
H = [âˆ‚Â²f/âˆ‚xÂ²    âˆ‚Â²f/âˆ‚xâˆ‚y]   [2  0]
    [âˆ‚Â²f/âˆ‚yâˆ‚x   âˆ‚Â²f/âˆ‚yÂ²  ] = [0  8]

**Inverse Hessian:**
Hâ»Â¹ = [1/2   0  ]
      [0     1/8]

**Newton step:**
[x_new]   [4]       [1/2   0  ] [8 ]
[y_new] = [2] - [0     1/8] [16]

        = [4] - [4]
          [2]   [2]

        = **[0, 0]áµ€**

**Answer: Newton's method finds the global minimum (0, 0) in one step!**

This is because f is quadratic, and Newton's method is exact for quadratic functions.

---

### Challenge 2: Batch Gradient

**Data:**
- (xâ‚, yâ‚) = (1, 3)
- (xâ‚‚, yâ‚‚) = (2, 5)
- (xâ‚ƒ, yâ‚ƒ) = (3, 7)

**Model:** Å· = wx + b
**Loss for single point:** Láµ¢ = (yáµ¢ - wxáµ¢ - b)Â²

**Current parameters:** w = 1.5, b = 2 (from Exercise 7.1)

**Gradient for each point:**

For point i:
âˆ‚Láµ¢/âˆ‚w = -2xáµ¢(yáµ¢ - wxáµ¢ - b)
âˆ‚Láµ¢/âˆ‚b = -2(yáµ¢ - wxáµ¢ - b)

**Point 1:** (1, 3)
Å·â‚ = 1.5(1) + 2 = 3.5
âˆ‚Lâ‚/âˆ‚w = -2(1)(3 - 3.5) = -2(1)(-0.5) = 1
âˆ‚Lâ‚/âˆ‚b = -2(-0.5) = 1

**Point 2:** (2, 5)
Å·â‚‚ = 1.5(2) + 2 = 5
âˆ‚Lâ‚‚/âˆ‚w = -2(2)(5 - 5) = 0
âˆ‚Lâ‚‚/âˆ‚b = -2(0) = 0

**Point 3:** (3, 7)
Å·â‚ƒ = 1.5(3) + 2 = 6.5
âˆ‚Lâ‚ƒ/âˆ‚w = -2(3)(7 - 6.5) = -6(0.5) = -3
âˆ‚Lâ‚ƒ/âˆ‚b = -2(0.5) = -1

**Average gradient (mini-batch):**

âˆ‚L/âˆ‚w = (1 + 0 + (-3))/3 = -2/3 â‰ˆ **-0.667**
âˆ‚L/âˆ‚b = (1 + 0 + (-1))/3 = 0/3 = **0**

**Update (Î± = 0.1):**

w_new = 1.5 - 0.1(-0.667) = 1.5 + 0.0667 = **1.567**
b_new = 2 - 0.1(0) = **2**

---

### Challenge 3: Derive Backprop for 2-Layer Network

**Network:**
```
x â†’ zâ‚ = Wâ‚x + bâ‚ â†’ hâ‚ = Ïƒ(zâ‚) â†’ zâ‚‚ = Wâ‚‚hâ‚ + bâ‚‚ â†’ hâ‚‚ = Ïƒ(zâ‚‚) â†’ L = (hâ‚‚ - y)Â²
```

**Notation:**
- Ïƒ'(z) = Ïƒ(z)(1 - Ïƒ(z))
- Î´â‚‚ = âˆ‚L/âˆ‚zâ‚‚
- Î´â‚ = âˆ‚L/âˆ‚zâ‚

**Backward pass:**

**1. âˆ‚L/âˆ‚hâ‚‚:**
âˆ‚L/âˆ‚hâ‚‚ = 2(hâ‚‚ - y)

**2. âˆ‚L/âˆ‚zâ‚‚ = Î´â‚‚:**
Î´â‚‚ = (âˆ‚L/âˆ‚hâ‚‚)(âˆ‚hâ‚‚/âˆ‚zâ‚‚)
   = 2(hâ‚‚ - y) Â· Ïƒ'(zâ‚‚)
   = 2(hâ‚‚ - y) Â· hâ‚‚(1 - hâ‚‚)

**3. âˆ‚L/âˆ‚Wâ‚‚:**
âˆ‚L/âˆ‚Wâ‚‚ = Î´â‚‚ Â· hâ‚áµ€

**4. âˆ‚L/âˆ‚bâ‚‚:**
âˆ‚L/âˆ‚bâ‚‚ = Î´â‚‚

**5. âˆ‚L/âˆ‚hâ‚:**
âˆ‚L/âˆ‚hâ‚ = Wâ‚‚áµ€ Â· Î´â‚‚

**6. âˆ‚L/âˆ‚zâ‚ = Î´â‚:**
Î´â‚ = (âˆ‚L/âˆ‚hâ‚) âŠ™ Ïƒ'(zâ‚)
   = (Wâ‚‚áµ€Î´â‚‚) âŠ™ [hâ‚ âŠ™ (1 - hâ‚)]

(âŠ™ denotes element-wise multiplication)

**7. âˆ‚L/âˆ‚Wâ‚:**
âˆ‚L/âˆ‚Wâ‚ = Î´â‚ Â· xáµ€

**8. âˆ‚L/âˆ‚bâ‚:**
âˆ‚L/âˆ‚bâ‚ = Î´â‚

**Summary:**
```
Forward: x â†’ zâ‚ â†’ hâ‚ â†’ zâ‚‚ â†’ hâ‚‚ â†’ L
Backward: âˆ‚L/âˆ‚Wâ‚, âˆ‚L/âˆ‚bâ‚ â† Î´â‚ â† Î´â‚‚ â† âˆ‚L/âˆ‚hâ‚‚
```

This is the essence of backpropagation! ğŸ§ 

---

## Key Takeaways

1. **Chain rule** is fundamental - all backpropagation uses it
2. **Gradient** points in direction of steepest ascent; negative gradient descends
3. **For ML:** Most common pattern is âˆ‚Loss/âˆ‚weight = (prediction - target) Ã— input
4. **Jacobians** generalize gradients to vector-valued functions
5. **Hessians** describe curvature (convexity) of loss surface
6. **Always verify gradients numerically** when implementing new architectures!

Practice until computing gradients becomes second nature! ğŸš€
