<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CNN Visualization Tutorial - ML Roadmap</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <style>
        .markdown-body {
            line-height: 1.6;
        }
        .markdown-body h1 {
            font-size: 2.5rem;
            font-weight: 700;
            margin-top: 1.5rem;
            margin-bottom: 1rem;
            color: #1f2937;
            border-bottom: 3px solid #10b981;
            padding-bottom: 0.5rem;
        }
        .markdown-body h2 {
            font-size: 1.875rem;
            font-weight: 700;
            margin-top: 2rem;
            margin-bottom: 0.75rem;
            color: #374151;
        }
        .markdown-body h3 {
            font-size: 1.5rem;
            font-weight: 600;
            margin-top: 1.5rem;
            margin-bottom: 0.5rem;
            color: #4b5563;
        }
        .markdown-body p {
            margin-bottom: 1rem;
        }
        .markdown-body ul, .markdown-body ol {
            margin-left: 1.5rem;
            margin-bottom: 1rem;
        }
        .markdown-body li {
            margin-bottom: 0.5rem;
        }
        .markdown-body code {
            background-color: #f3f4f6;
            padding: 0.2rem 0.4rem;
            border-radius: 0.25rem;
            font-size: 0.875rem;
            font-family: 'Courier New', monospace;
        }
        .markdown-body pre {
            background-color: #1f2937;
            color: #f9fafb;
            padding: 1rem;
            border-radius: 0.5rem;
            overflow-x: auto;
            margin-bottom: 1rem;
        }
        .markdown-body pre code {
            background-color: transparent;
            color: #f9fafb;
            padding: 0;
        }
        .markdown-body blockquote {
            border-left: 4px solid #10b981;
            padding-left: 1rem;
            margin-left: 0;
            color: #6b7280;
            font-style: italic;
        }
        .markdown-body strong {
            font-weight: 700;
            color: #1f2937;
        }
        .markdown-body hr {
            border: 0;
            border-top: 2px solid #e5e7eb;
            margin: 2rem 0;
        }
    </style>
</head>
<body class="bg-gray-50">
    <div class="container mx-auto px-4 py-8 max-w-4xl">
        <div class="bg-white rounded-lg shadow-lg p-8">
            <!-- Header -->
            <div class="mb-8 pb-6 border-b-2 border-gray-200">
                <h1 class="text-4xl font-bold text-gray-900 mb-4">CNN Visualization Tutorial</h1>
                <div class="flex flex-wrap gap-4 text-sm">
                    <div class="flex items-center">
                        <span class="font-semibold text-gray-700 mr-2">‚è±Ô∏è Time:</span>
                        <span class="text-gray-600">3-4 hours</span>
                    </div>
                    <div class="flex items-center">
                        <span class="font-semibold text-gray-700 mr-2">üìä Difficulty:</span>
                        <span class="text-gray-600">Intermediate</span>
                    </div>
                </div>
                <p class="mt-4 text-gray-600">Learn how to visualize and understand what your CNN has learned using 5 essential techniques.</p>
            </div>

            <!-- Content -->
            <div id="content" class="markdown-body"></div>
        </div>
    </div>

    <script>
        const markdown = `# CNN Visualization Tutorial
**Learn how to visualize and understand what your CNN has learned**

This tutorial teaches you 5 key techniques for visualizing CNNs in a notebook-style format. Follow along by creating your own Jupyter notebook!

---

## Setup: Imports and Dependencies

Let's start by importing all the libraries we'll need:

\`\`\`python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import zoom

# Set random seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)

# Check if GPU is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')
\`\`\`

---

## Define the CNN Architecture

We'll create a simple CNN for MNIST classification:

\`\`\`python
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        # Convolutional layers
        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=0)  # 28x28 -> 26x26
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # 26x26 -> 13x13

        # Fully connected layers
        self.fc1 = nn.Linear(8 * 13 * 13, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        # Conv -> ReLU -> Pool
        x = self.conv1(x)
        conv_out = x  # Save for visualization
        x = F.relu(x)
        relu_out = x  # Save for visualization
        x = self.pool(x)
        pool_out = x  # Save for visualization

        # Flatten
        x = x.view(-1, 8 * 13 * 13)

        # FC layers
        x = F.relu(self.fc1(x))
        x = self.fc2(x)

        return x

    def forward_with_activations(self, x):
        """Forward pass that returns intermediate activations"""
        conv_out = self.conv1(x)
        relu_out = F.relu(conv_out)
        pool_out = self.pool(relu_out)

        x = pool_out.view(-1, 8 * 13 * 13)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)

        return x, conv_out, relu_out, pool_out

# Create model
model = SimpleCNN().to(device)
print(model)
\`\`\`

---

## Load MNIST Dataset

\`\`\`python
# First, load dataset without normalization to compute statistics
temp_transform = transforms.ToTensor()
temp_dataset = datasets.MNIST('./data', train=True, download=True, transform=temp_transform)

# Compute mean and std of the training set
print("Computing dataset statistics...")
data_loader = DataLoader(temp_dataset, batch_size=1000, shuffle=False)

mean = 0.
std = 0.
nb_samples = 0.

for data, _ in data_loader:
    batch_samples = data.size(0)
    data = data.view(batch_samples, data.size(1), -1)  # Flatten spatial dimensions
    mean += data.mean(2).sum(0)
    std += data.std(2).sum(0)
    nb_samples += batch_samples

mean /= nb_samples
std /= nb_samples

print(f"Dataset mean: {mean.item():.4f}")
print(f"Dataset std: {std.item():.4f}")

# Now create datasets with computed normalization
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((mean.item(),), (std.item(),))
])

# Load datasets with normalization
train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST('./data', train=False, transform=transform)

# Create data loaders
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)

print(f'\\nTraining samples: {len(train_dataset)}')
print(f'Test samples: {len(test_dataset)}')
\`\`\`

**Note**: For MNIST, this will compute approximately mean=0.1307 and std=0.3081. These values normalize pixel values (0-1 after ToTensor) to have mean‚âà0 and std‚âà1, which helps with training stability.

---

## Train the CNN

\`\`\`python
def train_epoch(model, device, train_loader, optimizer, epoch):
    model.train()
    total_loss = 0
    correct = 0

    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)

        optimizer.zero_grad()
        output = model(data)
        loss = F.cross_entropy(output, target)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        pred = output.argmax(dim=1, keepdim=True)
        correct += pred.eq(target.view_as(pred)).sum().item()

        if batch_idx % 200 == 0:
            print(f'Epoch {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}] '
                  f'Loss: {loss.item():.4f}')

    avg_loss = total_loss / len(train_loader)
    accuracy = 100. * correct / len(train_loader.dataset)
    print(f'\\nEpoch {epoch} - Avg Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\\n')
    return avg_loss, accuracy

def test(model, device, test_loader):
    model.eval()
    test_loss = 0
    correct = 0

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += F.cross_entropy(output, target, reduction='sum').item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader.dataset)
    accuracy = 100. * correct / len(test_loader.dataset)
    print(f'Test - Avg Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%\\n')
    return test_loss, accuracy

# Train the model
optimizer = optim.Adam(model.parameters(), lr=0.001)

print("Training CNN...")
for epoch in range(1, 4):  # 3 epochs
    train_loss, train_acc = train_epoch(model, device, train_loader, optimizer, epoch)
    test_loss, test_acc = test(model, device, test_loader)

print("Training complete!")
\`\`\`

---

## 1. Visualizing Learned Filters (Kernels)

**What it shows**: The actual 3√ó3 weight matrices your CNN learned

**Why useful**: Reveals what features the conv layer detects (edges, corners, textures)

### Extract and visualize the filters:

\`\`\`python
# Get conv1 weights: shape (out_channels, in_channels, height, width)
# For our CNN: (8, 1, 3, 3) - 8 filters, 1 input channel, 3x3 kernel
conv_weights = model.conv1.weight.data.cpu()
print(f"Conv1 weights shape: {conv_weights.shape}")

# Visualize all 8 filters
fig, axes = plt.subplots(2, 4, figsize=(12, 6))
fig.suptitle('Learned 3x3 Convolutional Filters', fontsize=16, fontweight='bold')

for i in range(8):
    ax = axes[i // 4, i % 4]
    filter_i = conv_weights[i, 0].numpy()  # Shape: (3, 3)

    # Use RdBu colormap: red=positive, blue=negative
    max_abs = np.abs(filter_i).max()
    im = ax.imshow(filter_i, cmap='RdBu', interpolation='nearest',
                   vmin=-max_abs, vmax=max_abs)
    ax.set_title(f'Filter {i+1}', fontweight='bold')
    ax.axis('off')
    plt.colorbar(im, ax=ax, fraction=0.046)

plt.tight_layout()
plt.savefig('learned_filters.png', dpi=150, bbox_inches='tight')
plt.show()

print("‚úì Saved as 'learned_filters.png'")
\`\`\`

**What to look for**:
- Edge detectors: strong horizontal/vertical/diagonal patterns
- Blob detectors: center has opposite sign from edges
- Some filters may look random (not all filters learn useful features)

---

## 2. Visualizing Feature Maps (Activations)

**What it shows**: How each filter responds to a specific input image

**Why useful**: Shows what features the CNN extracts from different digits

### Get feature maps for a sample image:

\`\`\`python
# Pick a sample from test set
sample_idx = 0  # Try different indices!
image, label = test_dataset[sample_idx]
image = image.unsqueeze(0).to(device)  # Add batch dimension

print(f"Analyzing digit: {label}")
print(f"Image shape: {image.shape}")  # torch.Size([1, 1, 28, 28])

# Forward pass with activations
model.eval()
with torch.no_grad():
    output, conv_out, relu_out, pool_out = model.forward_with_activations(image)

# Move to CPU for visualization
conv_out = conv_out.cpu().squeeze(0)  # Remove batch dim -> (8, 26, 26)
relu_out = relu_out.cpu().squeeze(0)  # (8, 26, 26)
image_np = image.cpu().squeeze().numpy()  # (28, 28)

print(f"Feature maps shape: {relu_out.shape}")  # (8, 26, 26)
\`\`\`

### Visualize the feature maps:

\`\`\`python
fig, axes = plt.subplots(3, 3, figsize=(12, 12))
fig.suptitle(f'Feature Maps for Digit {label}', fontsize=16, fontweight='bold')

# Show original image
axes[0, 0].imshow(image_np, cmap='gray')
axes[0, 0].set_title('Original Image', fontweight='bold')
axes[0, 0].axis('off')

# Show first 8 feature maps (after ReLU)
for i in range(8):
    ax = axes[(i+1) // 3, (i+1) % 3]
    feature_map = relu_out[i].numpy()
    im = ax.imshow(feature_map, cmap='viridis')
    ax.set_title(f'Filter {i+1}', fontweight='bold')
    ax.axis('off')
    plt.colorbar(im, ax=ax, fraction=0.046)

plt.tight_layout()
plt.savefig(f'feature_maps_digit_{label}.png', dpi=150, bbox_inches='tight')
plt.show()

print(f"‚úì Saved as 'feature_maps_digit_{label}.png'")
\`\`\`

**What to look for**:
- Different filters activate on different digit features
- A "3" might strongly activate edge detectors on curves
- A "1" might activate vertical edge detectors
- Compare the same digit vs different digits

**Exercise**: Visualize feature maps for digits 0, 3, and 7. Notice how different filters activate differently!

---

## 3. Max-Activating Patches

**What it shows**: Which image patches cause each filter to activate most strongly

**Why useful**: Reveals what visual pattern each filter is "looking for"

### Find max-activating patches:

\`\`\`python
def find_max_activating_patches(model, dataset, filter_idx, top_k=9, num_search=1000):
    """Find patches that maximally activate a specific filter"""
    max_activations = []

    print(f"Searching for patches that activate Filter {filter_idx+1}...")

    model.eval()
    with torch.no_grad():
        for img_idx in range(min(num_search, len(dataset))):
            if (img_idx + 1) % 200 == 0:
                print(f"  Processed {img_idx+1}/{num_search} images...")

            img, _ = dataset[img_idx]
            img_batch = img.unsqueeze(0).to(device)

            # Get feature maps
            _, conv_out, relu_out, _ = model.forward_with_activations(img_batch)
            relu_out = relu_out.cpu().squeeze(0)  # (8, 26, 26)

            # Get max activation for this filter
            max_val = relu_out[filter_idx].max().item()

            if max_val > 0:
                # Find WHERE the max occurred
                pos = torch.argmax(relu_out[filter_idx]).item()
                h = pos // 26
                w = pos % 26
                max_activations.append((max_val, img.squeeze().numpy(), (h, w), img_idx))

    # Sort by activation strength
    max_activations.sort(reverse=True, key=lambda x: x[0])
    print(f"  Found {len(max_activations)} activating patches")
    return max_activations[:top_k]

# Get top 9 patches for filter 0 (try different filter_idx!)
filter_idx = 0
patches = find_max_activating_patches(model, test_dataset, filter_idx=filter_idx, top_k=9)
\`\`\`

### Visualize the patches:

\`\`\`python
fig, axes = plt.subplots(3, 3, figsize=(10, 10))
fig.suptitle(f'Top Patches Activating Filter {filter_idx+1}', fontsize=16, fontweight='bold')

for i, (activation, img, pos, img_idx) in enumerate(patches):
    ax = axes[i // 3, i % 3]

    # Show the full image
    ax.imshow(img, cmap='gray')

    # Highlight the 3x3 receptive field that caused the activation
    h, w = pos
    rect = plt.Rectangle((w, h), 3, 3,
                         linewidth=2, edgecolor='red', facecolor='none')
    ax.add_patch(rect)

    ax.set_title(f'Act: {activation:.2f} (img {img_idx})', fontsize=10)
    ax.axis('off')

plt.tight_layout()
plt.savefig(f'max_patches_filter_{filter_idx+1}.png', dpi=150, bbox_inches='tight')
plt.show()

print(f"‚úì Saved as 'max_patches_filter_{filter_idx+1}.png'")
\`\`\`

**What to look for**:
- Do all patches share a common pattern (edge, corner, curve)?
- Are they from the same digit or different digits?
- Does the filter specialize in one type of feature?

---

## 4. Activation Heatmaps

**What it shows**: Where the CNN "pays attention" in an image

**Why useful**: Shows which parts of the digit are important for classification

### Create activation heatmap:

\`\`\`python
# Pick an image from test set
sample_idx = 42  # Try different indices!
image, label = test_dataset[sample_idx]
image_batch = image.unsqueeze(0).to(device)

# Get activations
model.eval()
with torch.no_grad():
    _, conv_out, relu_out, _ = model.forward_with_activations(image_batch)

# Sum across all filters to get total activation
relu_out = relu_out.cpu().squeeze(0)  # (8, 26, 26)
activation_sum = torch.sum(relu_out, dim=0).numpy()  # (26, 26)
image_np = image.squeeze().numpy()  # (28, 28)

print(f"Digit: {label}")
print(f"Heatmap shape: {activation_sum.shape}")  # (26, 26)

# Resize heatmap to match original image size (28x28)
activation_resized = zoom(activation_sum, 28/26, order=1)
\`\`\`

### Visualize the heatmap:

\`\`\`python
fig, axes = plt.subplots(1, 3, figsize=(15, 5))
fig.suptitle(f'Activation Heatmap for Digit {label}', fontsize=16, fontweight='bold')

# 1. Original image
axes[0].imshow(image_np, cmap='gray')
axes[0].set_title('Original Image', fontsize=14, fontweight='bold')
axes[0].axis('off')

# 2. Heatmap alone
im = axes[1].imshow(activation_resized, cmap='hot', interpolation='bilinear')
axes[1].set_title('Activation Heatmap', fontsize=14, fontweight='bold')
axes[1].axis('off')
plt.colorbar(im, ax=axes[1], fraction=0.046)

# 3. Overlay on original
axes[2].imshow(image_np, cmap='gray', alpha=0.6)
axes[2].imshow(activation_resized, cmap='hot', alpha=0.4, interpolation='bilinear')
axes[2].set_title('Overlay', fontsize=14, fontweight='bold')
axes[2].axis('off')

plt.tight_layout()
plt.savefig(f'heatmap_digit_{label}.png', dpi=150, bbox_inches='tight')
plt.show()

print(f"‚úì Saved as 'heatmap_digit_{label}.png'")
\`\`\`

**What to look for**:
- CNN should activate strongly on edges and curves of the digit
- Background should have low activation
- Compare different digits - does CNN focus on different regions?

**Note**: The heatmap is 26√ó26 but the image is 28√ó28 because convolution reduces size. You may want to resize the heatmap for better overlay:
\`\`\`python
from scipy.ndimage import zoom
activation_resized = zoom(activation_sum, 28/26, order=1)
\`\`\`

---

## 5. Weight Distributions

**What it shows**: Statistical properties of learned weights

**Why useful**: Sanity check that training worked properly

### Analyze weight statistics:

\`\`\`python
# Extract all weights
conv_weights = model.conv1.weight.data.cpu().numpy().flatten()
fc1_weights = model.fc1.weight.data.cpu().numpy().flatten()
fc2_weights = model.fc2.weight.data.cpu().numpy().flatten()

print("=== Weight Statistics ===")
print(f"Conv weights - mean: {conv_weights.mean():.4f}, std: {conv_weights.std():.4f}")
print(f"             - min: {conv_weights.min():.4f}, max: {conv_weights.max():.4f}")
print(f"\\nFC1 weights  - mean: {fc1_weights.mean():.4f}, std: {fc1_weights.std():.4f}")
print(f"             - min: {fc1_weights.min():.4f}, max: {fc1_weights.max():.4f}")
print(f"\\nFC2 weights  - mean: {fc2_weights.mean():.4f}, std: {fc2_weights.std():.4f}")
print(f"             - min: {fc2_weights.min():.4f}, max: {fc2_weights.max():.4f}")
\`\`\`

### Visualize weight distributions:

\`\`\`python
fig, axes = plt.subplots(1, 3, figsize=(16, 4))
fig.suptitle('Weight Distributions After Training', fontsize=16, fontweight='bold')

# Conv weights histogram
axes[0].hist(conv_weights, bins=50, color='steelblue', alpha=0.7, edgecolor='black')
axes[0].set_title('Conv1 Weights', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Weight Value', fontsize=12)
axes[0].set_ylabel('Frequency', fontsize=12)
axes[0].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero')
axes[0].legend(fontsize=12)
axes[0].grid(alpha=0.3)

# FC1 weights histogram
axes[1].hist(fc1_weights, bins=50, color='coral', alpha=0.7, edgecolor='black')
axes[1].set_title('FC1 Weights', fontsize=14, fontweight='bold')
axes[1].set_xlabel('Weight Value', fontsize=12)
axes[1].set_ylabel('Frequency', fontsize=12)
axes[1].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero')
axes[1].legend(fontsize=12)
axes[1].grid(alpha=0.3)

# FC2 weights histogram
axes[2].hist(fc2_weights, bins=50, color='mediumseagreen', alpha=0.7, edgecolor='black')
axes[2].set_title('FC2 Weights', fontsize=14, fontweight='bold')
axes[2].set_xlabel('Weight Value', fontsize=12)
axes[2].set_ylabel('Frequency', fontsize=12)
axes[2].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero')
axes[2].legend(fontsize=12)
axes[2].grid(alpha=0.3)

plt.tight_layout()
plt.savefig('weight_distributions.png', dpi=150, bbox_inches='tight')
plt.show()

print("‚úì Saved as 'weight_distributions.png'")
\`\`\`

**What to look for**:
- Weights should be roughly centered around 0
- Should look somewhat Gaussian (bell curve)
- No extreme outliers (weights > 1.0 or < -1.0)
- If all weights are near 0, model didn't learn much
- If weights are huge, you may have exploding gradients

**Red flags**:
- Mean far from 0 ‚Üí possible bias in updates
- Very large std (> 0.5) ‚Üí possible instability
- Bimodal distribution ‚Üí something weird happened

---

## Putting It All Together

### Suggested workflow:

1. **Train your CNN** on MNIST until it reaches decent accuracy (>90%)

2. **Visualize filters** to see what features were learned

3. **Visualize feature maps** for different digits:
   - Pick 3-5 different digits
   - See which filters activate for each digit
   - Notice patterns (e.g., "filter 2 always activates on vertical edges")

4. **Find max-activating patches** for 2-3 interesting filters:
   - Do they consistently find edges? curves? corners?

5. **Create heatmaps** for correct and incorrect predictions:
   - Does CNN focus on the right parts when correct?
   - Does it focus on weird parts when wrong?

6. **Check weight distributions**:
   - Sanity check that training was healthy

---

## Bonus: Saliency Maps (Optional)

**What it shows**: Which input pixels most affect the output prediction

**Why useful**: Highlights the most important pixels for classification

### Compute saliency using gradients:

\`\`\`python
# Pick a sample
sample_idx = 10
image, label = test_dataset[sample_idx]
image_input = image.unsqueeze(0).to(device)
image_input.requires_grad = True  # Enable gradient computation

# Forward pass
model.eval()
output = model(image_input)
pred = output.argmax(dim=1).item()

print(f"True label: {label}, Predicted: {pred}")

# Backward pass to compute gradients w.r.t. input
model.zero_grad()
output[0, pred].backward()  # Gradient of predicted class score

# Get saliency map
saliency = image_input.grad.data.abs().cpu().squeeze().numpy()

# Visualize
fig, axes = plt.subplots(1, 3, figsize=(12, 4))
fig.suptitle(f'Saliency Map - True: {label}, Pred: {pred}', fontsize=14, fontweight='bold')

axes[0].imshow(image.squeeze(), cmap='gray')
axes[0].set_title('Original')
axes[0].axis('off')

axes[1].imshow(saliency, cmap='hot')
axes[1].set_title('Saliency Map')
axes[1].axis('off')

axes[2].imshow(image.squeeze(), cmap='gray', alpha=0.6)
axes[2].imshow(saliency, cmap='hot', alpha=0.4)
axes[2].set_title('Overlay')
axes[2].axis('off')

plt.tight_layout()
plt.savefig('saliency_map.png', dpi=150, bbox_inches='tight')
plt.show()

print("‚úì Saved as 'saliency_map.png'")
\`\`\`

---

## Summary and Key Takeaways

Congratulations! You've learned 5 essential techniques for visualizing CNNs:

1. **Learned Filters** ‚Üí See what low-level features the CNN detects (edges, corners, textures)
2. **Feature Maps** ‚Üí Understand how different filters activate for different inputs
3. **Max-Activating Patches** ‚Üí Discover what visual patterns each filter "looks for"
4. **Activation Heatmaps** ‚Üí Visualize where the CNN "pays attention"
5. **Weight Distributions** ‚Üí Sanity check that training worked properly

### Key Insights:

- ‚úÖ **Filters learn meaningful features** - not random noise
- ‚úÖ **Different filters specialize** - some detect edges, others detect curves
- ‚úÖ **CNNs focus on relevant regions** - edges and key features, not background
- ‚úÖ **Healthy networks have centered weight distributions** - mean ‚âà 0, modest std
- ‚úÖ **Visualization helps debugging** - if visualizations look random, something's wrong!

### Suggested Next Steps:

1. Compare visualizations before and after training - see how filters evolve
2. Analyze both correct and incorrect predictions - what's different?
3. Try different CNN architectures - how do deeper networks differ?
4. Experiment with other datasets (CIFAR-10, Fashion-MNIST)
5. Implement Grad-CAM for better localization

## References

- **CS231n Lecture 12**: [Visualizing and Understanding](https://cs231n.github.io/understanding-cnn/)
- **Zeiler & Fergus (2014)**: "Visualizing and Understanding Convolutional Networks"
- **Distill.pub**: [Feature Visualization](https://distill.pub/2017/feature-visualization/)
- **Grad-CAM Paper**: [Grad-CAM: Visual Explanations from Deep Networks](https://arxiv.org/abs/1610.02391)

---

**Now create your own Jupyter notebook and code along!** üé®
`;
        document.getElementById('content').innerHTML = marked.parse(markdown);
    </script>
</body>
</html>
