# Module 3: Optimization

**Focus:** Gradient descent, SGD, momentum, Adam

## ðŸŽ¯ Coding Tasks

### Task 1: Gradient Descent Variants
- [ ] Vanilla gradient descent
- [ ] Stochastic gradient descent (SGD)
- [ ] Mini-batch gradient descent
- [ ] Compare convergence

**Files to create:**
- `gradient_descent.py`
- `sgd_comparison.ipynb` (for visualizations)

---

### Task 2: Momentum-based Optimizers
- [ ] SGD with momentum
- [ ] RMSprop
- [ ] Adam optimizer
- [ ] Compare on test function

**Files to create:**
- `optimizers.py`
- `optimizer_comparison.ipynb`

---

### Project: MNIST Optimizer Comparison
- [ ] Train simple model with each optimizer
- [ ] Plot loss curves
- [ ] Compare convergence speed
- [ ] Analyze learning rate sensitivity

**Files to create:**
- `mnist_optimizer_comparison.ipynb`

---

## âœ… Success Criteria

- All optimizers implemented correctly
- Clear understanding of momentum/adaptive learning
- MNIST comparison shows expected behavior

## ðŸ“š Resources

- [Optimization Coding Guide](../guides/optimization_coding_guide.md)
- [Optimization Exercises](../guides/exercises/optimization_exercises.html)
- [Solutions](../guides/solutions/optimization_solutions.html)

---

**Time estimate:** 6-8 hours
